---
title: "Data 621 - Homework 3"
author: "Dhairav Chhatbar, Mael Illien, Salma Elshahawy"
date: "10/15/2020"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
# Data 621 Homework3 

## Introduction 

For this project, we built a binary logistic regression model from a dataset containing information on crime in various neighborhoods of a major city. Given a vector of predictors, we seek to predict whether the neighborhood crime rate is above the median. 


```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(skimr)
library(ggplot2)
library(ggcorrplot)
library(tidyverse)
library(PerformanceAnalytics)
library(DMwR)
library(caret)
library(kableExtra)
library(DescTools)
library(cowplot)
library(pROC)
```

```{r message=FALSE, warning=FALSE}
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-training-data_modified.csv", header = TRUE)
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-evaluation-data_modified.csv", header = TRUE)
```

## Data Exploration {.tabset .tabset-fade .tabset-pills}

### Data Exploration

The dataset is composed of 466 observations and 12 predictor variables. The response variable `target` is binary (0 or 1). A quick look at the distribution of the training dataset reveals some skewed predictors. All observations in this dataset are complete.

```{r data_summary_train, message=FALSE, warning=FALSE}
skim(data_train)
```

---

### Visualization

With a closer look at the distribution of the data using density plots, we can identify bimodal distribution for `indus`, `rar` and `tax` and skew in `dis`, `nox`, `ptratio` and `zn`.

```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  select(-target) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

#### Correlations with Response Variable

Comment on correlation matrix

```{r correlations_plot, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
q <- cor(data_train)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 
```

```{r}
#pairs(data_train, col=data_train$target)
```


---

## Data Preparation {.tabset .tabset-fade .tabset-pills}



---

### Data Transformation 



---
 
### Feature Engineering 


---

## Model Building {.tabset .tabset-fade .tabset-pills}

```{r}
# Initialize a df that will store the metrics of models
models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```


```{r}
# A function to extract the relevant metrics from the summary and confusion matrix
build_model <- function(id, formula, data) {
  glm.fit <- glm(formula, data=data, family=binomial)
  print(summary(glm.fit))
  glm.probs <- predict(glm.fit, type="response")
  # Confirm the 0.5 threshold
  glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
  results <- tibble(target=data_train$target, pred=glm.pred)
  results <- results %>%
    mutate(pred.class = as.factor(pred), target.class = as.factor(target))
  
  #print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
  
  acc <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$overall['Accuracy']
  sens <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Sensitivity']
  spec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Specificity']
  prec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Precision']
  res.deviance <- glm.fit$deviance
  null.deviance <- glm.fit$null.deviance  
  aic <- glm.fit$aic
  metrics <- list(res.deviance=res.deviance, null.deviance=null.deviance,aic=aic, accuracy=acc, sensitivity=sens, specificity=spec, precision=prec)
  metrics <- lapply(metrics, round, 3)
  
  plot(roc(results$target.class,glm.probs), print.auc = TRUE)
  model.df <- tibble(id=id, formula=formula, res.deviance=metrics$res.deviance, null.deviance=metrics$null.deviance, 
                         aic=metrics$aic, accuracy=metrics$accuracy, sensitivity=metrics$sensitivity, specificity=metrics$specificity, precision=metrics$precision)
  model.list <- list(model=glm.fit, df_info=model.df)
  #print(model.df)
  
  #models.df <- rbind(models.df,model.df)
  return(model.list)
}
```

### Model_1.1 Base Model Subsets

```{r}
# glm.fit1 <- glm(target ~ ., data=data_train, family=binomial)
# summary(glm.fit1)
m1 <- build_model('m1', "target ~ .", data = data_train)
m1$df_info
models.df <- rbind(models.df,m1$df_info)
```

```{r}
# glm.fit2 <- glm(target ~ . -rm, data=data_train, family=binomial)
# summary(glm.fit2)

m2 <- build_model('m2', "target ~ . -rm", data = data_train)
m2$df_info
models.df <- rbind(models.df,m2$df_info)
```
```{r}
# glm.fit3 <- glm(target ~ . -rm -chas, data=data_train, family=binomial)
# summary(glm.fit3)
m3 <- build_model('m3', "target ~ . -rm", data = data_train)
m3$df_info
models.df <- rbind(models.df,m3$df_info)
```

```{r}
# glm.fit4 <- glm(target ~ . -rm -chas -indus, data=data_train, family=binomial)
# summary(glm.fit4)
m4 <- build_model('m4', "target ~ . -rm -chas -indus", data = data_train)
m4$df_info
models.df <- rbind(models.df,m4$df_info)
```

```{r}
# glm.fit5 <- glm(target ~ . -rm -chas -indus -lstat, data=data_train, family=binomial)
# summary(glm.fit5)
m5 <- build_model('m5', "target ~ . -rm -chas -indus -lstat", data = data_train)
m5$df_info
models.df <- rbind(models.df,m5$df_info)
```

```{r}
models.df
```


### Model_1.2 

### Model_1.3 Transformations

Here we added transformed predictors with significant skew to the base model and iteratively eliminated features that were not statistically significant.

```{r}
trans_models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```

We proceed to remove the variables that were identified earlier to have low correlation or significant collinearity.

```{r}
data_train_trans <- data_train %>% select(-chas,-rm,-lstat)
```

```{r warning=FALSE}
mt1 <- build_model("mt1" ,"target ~ . + log(dis)+log(nox)", data=data_train_trans)
mt1$df_info
trans_models.df  <- rbind(trans_models.df, mt1$df_info)
```

We see that the transformed predictors that were introduced are collinear with the original predictors. To deal with this issue we remove the original predictors and proceed.

```{r}
car::vif(mt1$model)
```


```{r}
mt2 <- build_model("mt2","target ~ . + log(dis)+log(nox)-dis-nox", data=data_train_trans)
mt2$df_info
trans_models.df  <- rbind(trans_models.df, mt2$df_info)
```


```{r}
mt3 <- build_model("mt3","target ~ . + log(dis)+log(nox)-dis-nox-indus", data=data_train_trans)
mt3$df_info
trans_models.df  <- rbind(trans_models.df, mt3$df_info)
```

```{r}
mt4 <- build_model("mt4","target ~ . + log(dis)+log(nox)-dis-nox-indus-zn", data=data_train_trans)
mt4$df_info
trans_models.df  <- rbind(trans_models.df, mt4$df_info)
```



```{r}
# glm.fit5 <- glm(target ~ . -rm -chas -indus -lstat, data=data_train, family=binomial)
# summary(glm.fit5)
# mt5 <- build_model("mt5","target ~ . + log(dis)+log(nox)-indus-chas-rm-lstat", data=data_train)
# mt5$df_info
# trans_models.df  <- rbind(trans_models.df, mt5$df_info)
```


```{r}
#drop1(mt3$model,test="Chi")
```




## Model Selection 

When comparing the full model to its subsets we see that residual deviance tends to increase as predictors are dropped which indicates that the smaller models explain more of the residuals. Additionally, AIC also decreases meaning that less infomation is lost when using the smaller models. There is a minor accuracy penalty of 0.04% on the testing data when using the smaller model, which is acceptable. 

```{r}
models.df
```



```{r}
trans_models.df
```

The models that used additional transformed features had different results compared to the observations above. This time, both residual deviance and AIC seem to increase with smaller models. 

We see that mt1 which replaced the predictors `nox` and `dis` with their log transform has the highest accuracy (93.1%) and the lowest AIC (200.19) of the models tested so far. In addition, model mt1 delivers 1.5% more in accuracy than the best model subset without transformations.

Our preference for this model is comfirmed by a likelyhood ratio test. 

When comparing a larger to a smaller model, the null hypothesis is that the coefficients in the larger model are 0. In our case, the small p value suggests that the extra coefficients are not 0 and that we prefer the larger model. 

```{r}
anova(mt4$model, mt1$model, test = "LRT")
```

Taking a look at the coefficients and marginal effects, we can estimate the relative effects of each predictor on the odds of the target variable, keeping the other predictors the same.

```{r}
round(exp(mt4$model$coefficients),3)
```

```{r}
# Marginal effects
LogitScalar <- mean(dlogis(predict(mt4$model, type="link")))
LogitScalar * coef(mt3$model)
```

---
