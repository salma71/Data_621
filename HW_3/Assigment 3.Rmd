---
title: "Data 621 - Homework 3"
author: "Dhairav Chhatbar, Mael Illien, Salma Elshahawy"
date: "10/15/2020"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
# Data 621 Homework3 

## Introduction 


```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(skimr)
library(ggplot2)
library(ggcorrplot)
library(tidyverse)
library(PerformanceAnalytics)
library(DMwR)
library(caret)
library(kableExtra)
library(DescTools)
library(cowplot)
library(broom)
library(caret)
library(car)
```

```{r message=FALSE, warning=FALSE}
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-training-data_modified.csv", header = TRUE)
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-evaluation-data_modified.csv", header = TRUE)
```

## Data Exploration {.tabset .tabset-fade .tabset-pills}

### Data Exploration

```{r data_summary_train, message=FALSE, warning=FALSE}
skim(data_train)
```

---

### Missing Data

There is no missing data per the skim summary above.

---

### Visualization
Looking at the density and box plots of each we can observe that:
  
* **age**: there is a higher concentration of homes that are older (more than 50 years old). The box plot shows that generally older homes in neighborhoods are see as associated with higher crime.  
* **chas**: most homes in the dataset border the Charles River, thus this may not be a good predictor variable
* **dis**: a right skewed distribution where a lower distance to employment centers shows a higher crime indicator
* **indus**: bi-modual distribution of industrial sectors and generally seen by the box plots that the higher industrial activity results in an increased crime factor
* **lstat**: a predictor variable based on "status" of population. However it is ambigious what the sale in this factor reflects, but the observation is that the higher on the lstat scale the more indicator of crime
* **mdev**: median value of homes, and seems correct that we would see higher value homes associated with lower crimes
* **nox**: the amount of nitrogen oxides concentrations is right skewed with most locations not having a "high" amount, and as the concentration increases as does the crime
* **ptratio**: student to teacher ratio, as convention and observation show a high student to teacher ratio is indicative of higher crimes
* **rad**: the distance to highways seems slightly bi-modal, and higher distance from highways seems to be associated with higher crime, however the variability on the positive crime indicator is very large
* **rm**: the average number of rooms per home looks normally distributed and the association with crime seems evenly distributed as per the box plot
* **tax**: the property tax variable is bi-modal, the box plot shows that the variability of a positive crime indicator is fairly large
* **zn**: large lot zones show most values as 0 and lower proportions seems associated with higher crime

```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  select(-target) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r box_plot, warning=FALSE, message=FALSE, fig.height=8, fig.width=10}

data_train_long <- gather(data_train, "Variable", "Value", zn:medv)
data_train_long$target <- as.factor(data_train_long$target)  
ggplot(data_train_long, aes(x=target, y=Value)) + geom_boxplot(varwidth = TRUE, alpha=0.2, fill="orange1") + 
  facet_wrap(~Variable, scales = "free")
  

```

#### Correlations with Response Variable
Looking at the correlation plot of our data set we see the below and confirm some of the observations made from the density and box plots:
  
* the target variable is positively correlated with **nox(.73)**, **age(.63)**, **rad(.63)**, and **tax(.61)**
* the target variable is negatively correlated with **dis(-.62)**
* as seen in the density and box plots, the **chas** variable as a very weak correlation with all the other variables, and including the target. Therefore we can look to eliminate it from the analysis  
* There is present a amount of correlation amongst the predictor variables and this is suspect for multicollinearity issues  


Highest correlations amongst predictor variables:  
- tax|rad (.91)  
-nox|indus (.76)  
-age|nox(.74)  
-tax|indus (.73)  
-medv|rm (.71)  
-dis|indus (-.7)  
-medv|lstat (-.74)  
-dist|age (-.75)  
-dis|nox (-.77)  


```{r correlations_plot, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
q <- cor(data_train)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 
```


---

## Data Preparation {.tabset .tabset-fade .tabset-pills}
In preparation of the data to use in the model we remove the **chas** variable is it does not seem to have any impact on target and is not highly correlated with any other predictor variable.  
  
We also convert the taget variable into a factor from an integer type
```{r}
data_train_m <- data_train %>% select(-chas)

#convert target to factor
data_train_m$target <- as.factor(data_train_m$target)

```

### Influential Leverage Points
In preparation of the data to use in the model we will look to see if within the data set that are any bad influence points that may have a significant impact on the model.

From the general model below are some outlier observations based on Cooks distance, though not all outlier observations are influential
```{r}
ilp <- glm(target ~ .,data = data_train_m, family = binomial(link="logit"))

#Top 5 outliers
plot(ilp, which = 4, id.n = 5)
```

To see if any outlier observations are influential we plot the standardized residual error to determine if any residuals are above the absolute value of 3. 
```{r}
augment(ilp) %>% mutate(index = 1:n()) %>% ggplot( aes(index, .std.resid)) + geom_point(aes(color=target)) + labs(y= "Standardized Residuals", x="Observation Index")
```

From the above plot there are is 1 value that is greater than 3. Observation 457. We will remove them from our dataset, we will also remove observation 338 which is close to 3 but not quite
```{r}

(augment(ilp) %>% mutate(index = 1:n()) %>% top_n(2, .std.resid))[,c(1:9,15,19)]

#remove from model
data_train_m <- data_train_m[-c(457,338),]


```

---

### Data Transformation 



---
 
### Feature Engineering 


---

## Model Building {.tabset .tabset-fade .tabset-pills}

### Model_1.1 

```{r}
glm.fit1 <- glm(target ~ ., data=data_train, family=binomial)
summary(glm.fit1)
```

```{r}
glm.fit2 <- glm(target ~ . -rm, data=data_train, family=binomial)
summary(glm.fit2)
```
```{r}
glm.fit3 <- glm(target ~ . -rm -chas, data=data_train, family=binomial)
summary(glm.fit3)
```

```{r}
glm.fit4 <- glm(target ~ . -rm -chas -indus, data=data_train, family=binomial)
summary(glm.fit4)
```

```{r}
glm.fit5 <- glm(target ~ . -rm -chas -indus -lstat, data=data_train, family=binomial)
summary(glm.fit5)
```

```{r}
glm.probs <- predict(glm.fit5, type="response")
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
results <- tibble(target=data_train$target, pred=glm.pred)
results
```

```{r}
table(glm.pred,data_train$target)
```

```{r}
b <- results %>%
  mutate(pred.class = as.factor(pred), target.class = as.factor(target))
confusionMatrix(b$pred.class,b$target.class, positive = "1")
```

```{r}
library(pROC)
plot(roc(b$target.class,glm.probs), print.auc = TRUE)
```


### Model_1.2 
Model 1.2 is a model that has the below characteristics:  
  
* Predictor chas as been removed
* The target variable has been converted to a factor
* Observation 457 has been removed

In the above sections we indicated that we were concerned with multicollinearity issues with the predictor values. We look further into this by checking the **Variance Inflation Factor (VIF)**, where anything above a 5 is a high collinearity that may be problematic to the model. We see that the predictor **medv** has a VIF value of 9. 
```{r, fig.height=8, fig.width=10}

m1 <- glm(target ~., data = data_train_m, family = binomial(link="logit"))
summary(m1)

car::vif(m1)
```
The above model indicates that the predictor **mdev** can be problematic. **mdev** is highly correlated with **lstat** and **rm**, both of which are not significant in the model, but **mdev** is significant. When removing **lstat** and **rm** from the model we see that this addresses the collinearity issue with all VIF values under 5
#mdev is corrolated with lstat and rm, and both are not significant in the model m1. Remove them
```{r}
#remove rm and lstat from model
data_train_mv2 <- data_train_m %>% select(-rm, -lstat)

m1b <- glm(target ~., data = data_train_mv2, family = binomial(link="logit"))
summary(m1b)

car::vif(m1b)

```


Backward elimination produces the final model
```{r}
#continue backwards elimination
m1c <- update(m1b, . ~ . -indus)
summary(m1c)
```

In the Margininal model plot, we see that the expected values follow the observed values and therefore we can say that this model is adequate for the given data
```{r, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}

marginalModelPlots(m1c)
```



Accuracy check
```{r}
#Accuracy
m1c.probs <- predict(m1c, type="response")
m1c.pred <- ifelse(m1c.probs > 0.5, 1, 0)
m1c.results <- tibble(target=data_train_m$target, pred=m1c.pred)


w <- m1c.results %>%
  mutate(pred.class = as.factor(pred), target.class = as.factor(target))

confusionMatrix(w$pred.class,w$target.class, positive = "1")

plot(roc(w$target.class,m1c.probs), print.auc = TRUE)

```

### Model_1.3 

## Model Selection 

---