---
title: "Data 621 - Homework 3"
author: "Dhairav Chhatbar, Mael Illien, Salma Elshahawy"
date: "10/15/2020"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
# Data 621 Homework3 

## Introduction 

For this assignment, we were tasked with building a binary logistic regression model from a dataset containing information on crime in various neighborhoods of a major city. Given a vector of predictors, we seek to predict whether the neighborhood crime rate is above the median. 


```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(skimr)
library(ggcorrplot)
library(tidyverse)
library(PerformanceAnalytics)
library(DMwR)
library(caret)
library(kableExtra)
library(DescTools)
library(cowplot)
library(pROC)
library(broom)
library(car)
```

```{r message=FALSE, warning=FALSE}
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-training-data_modified.csv", header = TRUE)
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_3/crime-evaluation-data_modified.csv", header = TRUE)
```

## Data Exploration {.tabset .tabset-fade .tabset-pills}

### Data Exploration

The dataset is composed of 466 observations and 12 predictor variables. The response variable *target* is binary (0 or 1). A quick look at the distribution of the training dataset reveals some skewed predictors. All observations in this dataset are complete.

```{r data_summary_train, message=FALSE, warning=FALSE}
skim(data_train)
```

---

### Visualization

With a closer look at the distribution of the data using density plots, we can identify bimodal distribution for *indus*, *rar* and *tax* and skew in *dis*, *nox*, *ptratio* and *zn*.

Looking at the density and box plots of each we can observe that:

- *age*: there is a higher concentration of homes that are older (more than 50 years old). The box plot shows that generally older homes in neighborhoods are see as associated with higher crime.
- *chas*: most homes in the dataset border the Charles River, thus this may not be a good predictor variable
  *dis*: a right skewed distribution where a lower distance to employment centers shows a higher crime indicator
- *indus*: bi-modual distribution of industrial sectors and generally seen by the box plots that the higher industrial activity results in an increased crime factor
- *lstat*: a predictor variable based on “status” of population. However it is ambigious what the sale in this factor reflects, but the observation is that the higher on the lstat scale the more indicator of crime
- *mdev*: median value of homes, and seems correct that we would see higher value homes associated with lower crimes
- *nox*: the amount of nitrogen oxides concentrations is right skewed with most locations not having a “high” amount, and as the concentration increases as does the crime
- *ptratio*: student to teacher ratio, as convention and observation show a high student to teacher ratio is indicative of higher crimes
- *rad*: the distance to highways seems slightly bi-modal, and higher distance from highways seems to be associated with higher crime, however the variability on the positive crime indicator is very large
- *rm*: the average number of rooms per home looks normally distributed and the association with crime seems evenly distributed as per the box plot
- *tax*: the property tax variable is bi-modal, the box plot shows that the variability of a positive crime indicator is fairly large
- *zn*: large lot zones show most values as 0 and lower proportions seems associated with higher crime

```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  select(-target) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r, fig.height=8, fig.width=10}
data_train_long <- gather(data_train, "Variable", "Value", zn:medv)
data_train_long$target <- as.factor(data_train_long$target)  
ggplot(data_train_long, aes(x=target, y=Value)) + geom_boxplot(varwidth = TRUE, alpha=0.2, fill="orange1") + facet_wrap(~Variable, scales = "free")
```


#### Correlations with Response Variable

Looking at the correlation plot of our data set we see the below and confirm some of the observations made from the density and box plots:

- the target variable is positively correlated with *nox(.73), age(.63), rad(.63)*, and *tax(.61)*
- the target variable is negatively correlated with *dis(-.62)*
as seen in the density and box plots, the *chas* variable as a very weak correlation with all the other variables, and including the target. Therefore we can look to eliminate it from the analysis
- there is present a amount of correlation amongst the predictor variables and this is suspect for multicollinearity issues

Highest correlations amongst predictor variables:  
- tax|rad (.91)  
-nox|indus (.76)  
-age|nox(.74)  
-tax|indus (.73)  
-medv|rm (.71)  
-dis|indus (-.7)  
-medv|lstat (-.74)  
-dist|age (-.75)  
-dis|nox (-.77)  

```{r correlations_plot, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
q <- cor(data_train)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 
```

```{r}
#pairs(data_train, col=data_train$target)
```


---

## Data Preparation {.tabset .tabset-fade .tabset-pills}

In preparation of the data to use in the model we remove the chas variable is it does not seem to have any impact on target and is not highly correlated with any other predictor variable.

We also convert the target variable into a factor from an integer type

```{r}
data_train_m2 <- data_train %>% select(-chas)
#convert target to factor
data_train_m2$target <- as.factor(data_train_m2$target)
```

---

### Influential Leverage Points

In preparation of the data to use in the model we will look to see if within the data set that are any influence points that may have a significant impact on the model.

From the general model below are some outlier observations based on Cooks distance, though not all outlier observations are influential

```{r}
ilp <- glm(target ~ .,data = data_train_m2, family = binomial(link="logit"))
#Top 5 outliers
plot(ilp, which = 4, id.n = 5)
```

To see if any outlier observations are influential we plot the standardized residual error to determine if any residuals are above the absolute value of 3.


```{r}
augment(ilp) %>% mutate(index = 1:n()) %>% ggplot(aes(index, .std.resid)) + geom_point(aes(color=target)) + labs(y= "Standardized Residuals", x="Observation Index")
```

From the above plot there are is 1 value that is greater than 3. Observation 457. We will remove them from our dataset, we will also remove observation 338 which is close to 3 but not quite

```{r}
(augment(ilp) %>% mutate(index = 1:n()) %>% top_n(2, .std.resid))[,c(1:9,15,19)]
```

```{r}
#remove from model
data_train_m2 <- data_train_m2[-c(457,338),]
```


---

## Model Building {.tabset .tabset-fade .tabset-pills}

Models in this sections are simply subsets of the full model.

```{r}
# Initialize a df that will store the metrics of models
models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```


```{r}
# A function to extract the relevant metrics from the summary and confusion matrix
build_model <- function(id, formula, data) {
  glm.fit <- glm(formula, data=data, family=binomial)
  print(summary(glm.fit))
  glm.probs <- predict(glm.fit, type="response")
  # Confirm the 0.5 threshold
  glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
  results <- tibble(target=data$target, pred=glm.pred)
  results <- results %>%
    mutate(pred.class = as.factor(pred), target.class = as.factor(target))
  
  #print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
  
  acc <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$overall['Accuracy']
  sens <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Sensitivity']
  spec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Specificity']
  prec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Precision']
  res.deviance <- glm.fit$deviance
  null.deviance <- glm.fit$null.deviance  
  aic <- glm.fit$aic
  metrics <- list(res.deviance=res.deviance, null.deviance=null.deviance,aic=aic, accuracy=acc, sensitivity=sens, specificity=spec, precision=prec)
  metrics <- lapply(metrics, round, 3)
  
  plot(roc(results$target.class,glm.probs), print.auc = TRUE)
  model.df <- tibble(id=id, formula=formula, res.deviance=metrics$res.deviance, null.deviance=metrics$null.deviance, 
                         aic=metrics$aic, accuracy=metrics$accuracy, sensitivity=metrics$sensitivity, specificity=metrics$specificity, precision=metrics$precision)
  model.list <- list(model=glm.fit, df_info=model.df)
  return(model.list)
}
```

### Model 1: Base Model Subsets

Model 1 starts with a complete model including all predictors iteratively finds the model with the subset of predictors that are statistically significant.

```{r, warning=FALSE, message=FALSE}
m1 <- build_model('m1', "target ~ .", data = data_train)


m1$df_info
models.df <- rbind(models.df,m1$df_info)
```

```{r}
m1_b <- build_model('m1_b', "target ~ . -rm", data = data_train)
m1_b$df_info
models.df <- rbind(models.df,m1_b$df_info)
```
```{r}
m1_c <- build_model('m1_c', "target ~ . -rm - chas", data = data_train)
m1_c$df_info
models.df <- rbind(models.df,m1_c$df_info)
```

```{r}
m1_d <- build_model('m1_d', "target ~ . -rm -chas -indus", data = data_train)
m1_d$df_info
models.df <- rbind(models.df,m1_d$df_info)
```

```{r}
m1_e <- build_model('m1_e', "target ~ . -rm -chas -indus -lstat", data = data_train)
m1_e$df_info
models.df <- rbind(models.df,m1_e$df_info)
```

Here is a summary of the models built in this section.

```{r}
models.df
```


### Model 2: Observation Removal

Model 2 is a model that has the below characteristics:

- Predictor chas as been removed
- The target variable has been converted to a factor
- Observations 338 and 457 have been removed as influencial outliers
- In the above sections we indicated that we were concerned with multicollinearity issues with the predictor values. We look further into this by checking the Variance Inflation Factor (VIF), where anything above a 5 is a high collinearity that may be problematic to the model. We see that the predictor medv has a VIF value of 9.

```{r}
m2 <- build_model("m2", "target ~ .", data = data_train_m2)
m2$df_info
models.df <- rbind(models.df,m2$df_info)
```

```{r}
car::vif(m2$model)
```

The above model indicates that the predictor mdev can be problematic. mdev is highly correlated with lstat and rm, both of which are not significant in the model, but mdev is significant. When removing lstat and rm from the model we see that this addresses the collinearity issue with all VIF values under 5

```{r}
#remove rm and lstat from model
m2_b <- build_model('m2_b', "target ~ . -rm - lstat", data = data_train_m2)

m2_b$df_info
models.df <- rbind(models.df,m2_b$df_info)

car::vif(m2_b$model)
```



Backward elimination produces the final model:

```{r}
#continue backwards elimination
#m1c <- build_model('m1c', "target ~ . -indus", data = data_train_mv2)
m2_c <- build_model('m2_c', "target ~ . -rm -lstat -indus", data = data_train_m2)
m2_c$df_info
models.df <- rbind(models.df,m2_c$df_info)
```

### Model 3: Transformations

Here we added transformed predictors with significant skew to the base model and iteratively eliminated features that were not statistically significant.

```{r}
trans_models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```

We proceed to remove the variables that were identified earlier to have low correlation or significant multicollinearity.

```{r}
data_train_trans <- data_train %>% select(-chas,-rm,-lstat)
```

```{r warning=FALSE}
mt1 <- build_model("mt1" ,"target ~ . + log(dis)+log(nox)", data=data_train_trans)
mt1$df_info
trans_models.df  <- rbind(trans_models.df, mt1$df_info)
```

We see that the transformed predictors that were introduced are collinear with the original predictors. To deal with this issue we remove the original predictors and proceed.

```{r}
car::vif(mt1$model)
```


```{r}
mt2 <- build_model("mt2","target ~ . + log(dis)+log(nox)-dis-nox", data=data_train_trans)
mt2$df_info
trans_models.df  <- rbind(trans_models.df, mt2$df_info)
```


```{r}
mt3 <- build_model("mt3","target ~ . + log(dis)+log(nox)-dis-nox-indus", data=data_train_trans)
mt3$df_info
trans_models.df  <- rbind(trans_models.df, mt3$df_info)
```

```{r}
mt4 <- build_model("mt4","target ~ . + log(dis)+log(nox)-dis-nox-indus-zn", data=data_train_trans)
mt4$df_info
trans_models.df  <- rbind(trans_models.df, mt4$df_info)
```



## Model Selection 

When comparing the full model to its subsets we see that residual deviance tends to increase as predictors are dropped which indicates that the smaller models explain more of the residuals. Additionally, AIC also decreases meaning that less information is lost when using the smaller models. There is a minor accuracy penalty of 0.04% on the testing data when using the smaller model, which is acceptable. 

```{r}
models.df
```

However, the models that used additional transformed features had different results compared to the observations above. This time, both residual deviance and AIC seem to increase with smaller models. 

```{r}
trans_models.df
```

We see that mt1 which replaced the predictors `nox` and `dis` with their log transform has the highest accuracy (93.1%) and the lowest AIC (200.19) of the models tested so far. In addition, model mt1 delivers 1.5% more in accuracy than the best model subset without transformations.

Our preference for this model is confirmed by a likelyhood ratio test. 

When comparing a larger to a smaller model, the null hypothesis is that the coefficients in the larger model are 0. In our case, the small p value suggests that the extra coefficients are not 0 and that we prefer the larger model. 

```{r}
anova(mt4$model, mt1$model, test = "LRT")
```

Therefore, our selected final model is mt1 which yields an accuracy on the test set of 93.1%.

Taking a look at the coefficients and marginal effects, we can now estimate the relative effects of each predictor on the odds of the target variable, keeping the other predictors the same.

```{r}
round(exp(mt4$model$coefficients),3)
```
Interestingly and counterintuitively, the predictor log(nox) seems to have the largest single affect on a neighborhood.
```{r}
# Marginal effects
LogitScalar <- mean(dlogis(predict(mt4$model, type="link")))
LogitScalar * coef(mt3$model)
```

Looking at the marginal model plots, we see that the for all variables the data follows the expected values of the model, though there is a slight variation in the variables indus and age but not significant. We can therefore say that this is the correct model for the given data

```{r, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}

marginalModelPlots(glm(target ~ . + log(dis)+log(nox), data=data_train_trans, family = binomial))


```


---