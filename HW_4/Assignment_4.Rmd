---
title: "Data 621 - Homework 4"
author: "Dhairav Chhatbar, Mael Illien, Salma Elshahawy"
date: "11/4/2020"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
# Data 621 Homework 4

## Introduction 

In this assignment, we will explore, analyze and model a data set containing approximately 8000 records, each representing a customer at an auto insurance company. Each record has two response variables. The first response variable, `TARGET_FLAG`, is binary. A “1” indicates that the customer was in a car crash while 0 indicates that they were not. The second response variable is `TARGET_AMT`. This value is 0 if the customer did not crash their car. However, if they did crash their car, this number will be a value greater than 0.

The objective is to build multiple linear regression and binary logistic regression models on the training data to predict whether a customer will crash their car and to predict the cost in the case of crash. We will only use the variables given to us (or variables that we derive from the variables provided). 

Below is a short description of the variables of interest in the data set:

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(skimr)
library(ggcorrplot)
library(tidyverse)
library(PerformanceAnalytics)
library(DMwR)
library(caret)
library(kableExtra)
library(DescTools)
library(cowplot)
library(pROC)
library(broom)
library(car)
library(jtools)
library(MASS)
```

```{r message=FALSE, warning=FALSE}
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_4/insurance_training_data.csv", header = TRUE)
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_4/insurance-evaluation-data.csv", header = TRUE)
```

```{r}
data_train
```

## Data Exploration {.tabset .tabset-fade .tabset-pills}

### Data Exploration

The dataset consists of **26** variables and **8161** observations with `AGE`, `YOJ`, and `CAR_AGE` variables containing some missing values. As stated previously, `TARGET_FLAG` and `TARGET_AMT` are our response variables. Also, `13` of the variables have discrete values and the rest of the variables are continuous. 

```{r data_summary_train, message=FALSE, warning=FALSE}
skim(data_train)
```

```{r}
data_train %>% summarize_all(funs(sum(is.na(.)) / length(.)))
```

## Data Processing {.tabset .tabset-fade .tabset-pills}

---

### Fix formatting

The currency notation found in some values will interfere with our analysis so we need reformat those values appropriately.

```{r}
strip_dollars <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}

fix_formatting <- function(messy_df){
  messy_df %>%
    rowwise() %>%
    mutate(INCOME = strip_dollars(INCOME),
           HOME_VAL = strip_dollars(HOME_VAL),
           BLUEBOOK = strip_dollars(BLUEBOOK),
           OLDCLAIM = strip_dollars(OLDCLAIM)) %>%
    ungroup()
}
```

### Fix data types

We noticed that a few variables that are listed as discrete have large numbers of unique values. A closer inspection of the variable descriptions reveals that that while these variables are encoded as factors they are actually continuous. The `TARGET_FLAG` variable also appears in the summary as numeric variable, but it should be a binary factor. We proceed to fix these data types.

```{r}
fix_data_types <- function(messy_df){
  messy_df %>%
    rowwise() %>%
    mutate(INCOME = as.numeric(INCOME),
           HOME_VAL = as.numeric(HOME_VAL),
           BLUEBOOK = as.numeric(BLUEBOOK),
           OLDCLAIM = as.numeric(OLDCLAIM)) %>%
    ungroup()
}

data_train$TARGET_FLAG <- factor(data_train$TARGET_FLAG)
```

### Fix bad and missing values

Also, there are some values that seem invalid (i.e. -3 `CAR_AGE`). Since both variables the missing values are less than 5% then we can replace the missing values with the median. We Will take the median on the training set only and impute in both training and testing to avoid overfitting. 

```{r}
na_bad_values <- function(messy_df){
  messy_df %>%
    rowwise() %>%
    mutate(CAR_AGE = ifelse(CAR_AGE < 0, NA, CAR_AGE))%>%
    ungroup()
}

fix_missing <- function(df) {
  df %>% 
    mutate_at(vars(c("CAR_AGE", "YOJ", "AGE", "INCOME", "HOME_VAL")), ~ifelse(is.na(.), median(., na.rm = TRUE), .))
}
```

### Process data

We apply the processing steps above to both the training and testing datasets. 

```{r}
data_train <- data_train %>%
  fix_formatting() %>%
  fix_data_types() %>%
  na_bad_values() %>%
  fix_missing()
```

```{r}
data_test <- data_test %>%
  fix_formatting() %>%
  fix_data_types() %>%
  na_bad_values() %>%
  fix_missing()
```

### Univariate charts

We now explore the distribution of `TARGET_FLAG` across the numeric variables. We see that `BLUEBOOK`, `INCOME`, `OLDCLAIM` have a high number of outliers compared to other variables. We also see that customers with who are older, or have older cars, higher home values, higher income tend to get into fewer car crashes. However, people with motor vehicle record points or high number of old claims tend to get into more accidents.

```{r fig.height=10, fig.width=10}
plot_vars <- c("TARGET_FLAG", names(keep(data_train, is.numeric)))

data_train[plot_vars] %>%
  dplyr::select(-INDEX, -TARGET_AMT) %>%
  gather(variable, value, -TARGET_FLAG) %>%
  ggplot(., aes(TARGET_FLAG, value, color=TARGET_FLAG)) + 
  geom_boxplot() +
  scale_color_brewer(palette="Set1") +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```


```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  dplyr::select(-TARGET_FLAG, -TARGET_AMT, -INDEX) %>%
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_histogram(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

The variables dislayed below need scale transformations like `OLDCLAIM`, `INCOME`, ` BLUEBOOK`, `HOME_VAL`.
`AGE`has a guassian distribution. We see several variables have high number of zeros. `AGE` is the only variable that is normally distributed. Rest of the variables show some skewness. We will perform Box-Cox transformation on these variables.

```{r variables_distribution2, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  dplyr::select(OLDCLAIM, INCOME, BLUEBOOK, HOME_VAL) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_histogram(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r fig.height=10, fig.width=10}
data_train %>%
  keep(is.numeric) %>%
  gather(variable, value, -TARGET_AMT, -INDEX, -CLM_FREQ, -MVR_PTS) %>%
  ggplot(., aes(value, TARGET_AMT)) + 
  geom_point() +
  scale_color_brewer(palette="Set1") +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(~variable, scales ="free", ncol = 3) +
  labs(x = element_blank(), y = element_blank())
```

### Correlation

We see `MVR_PTS`, `CLM_FREQ`, and `OLDCLAIM` are the most positively correlated variables with our response variables. Whereas, `URBANICITY` is the most negatively correlated variable. Rest of the variables are weakly correlated.

```{r correlations_plot, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
corr_dataframe = data_train %>%
    mutate_if(is.factor, as.numeric)
q <- cor(corr_dataframe)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 
```

### Centrality Measures and Outliers

```{r fig.height=5, fig.width=10}
set.seed(42)
accidents <- data_train %>%
  filter(TARGET_FLAG == 1)

ggplot(accidents, aes(x=TARGET_AMT)) + 
  geom_density(fill='pink') +
  theme_light() +
  geom_vline(aes(xintercept = mean(TARGET_AMT)), lty=2, col="red") +
  geom_label(aes(x=25000, y=0.00015, label=paste("mean =", round(mean(TARGET_AMT),0)))) +
  geom_vline(aes(xintercept = median(TARGET_AMT)), lty=2, col="darkgreen") +
  geom_label(aes(x=25000, y=0.00010, label=paste("median = ", round(median(TARGET_AMT), 0)))) +
  labs(title="TARGET_AMT Density Plot", y="Density", x="TARGET_AMT")
```

As was previously noted this distribution has a long tail. The mean payout is $5616 and the median is $4102. The median and mean are higher, of course for those observations we classified as outliers. The outlier cutoff point is $10594.

```{r}
outlier <- min(boxplot(data_train[data_train$TARGET_FLAG==1,]$TARGET_AMT, plot=FALSE)$out)
data_train %>%
  mutate(TARGET_AMT_OUTLIER = ifelse(TARGET_AMT < outlier, "Yes", "No")) %>%
  group_by(TARGET_AMT_OUTLIER) %>%
  summarise(Mean = mean(TARGET_AMT),
            Median = median(TARGET_AMT)) 
```



---

## Data Preparation {.tabset .tabset-fade .tabset-pills}

### Sampling

```{r}
table(data_train$TARGET_FLAG)
```

There is an imbalance in the `TARGET_FLAG` variable

Let's check the class distribution 

```{r}
prop.table(table(data_train$TARGET_FLAG))
```

The data contains only 26% that has already did an accident and 74% of negative flag. This is severly imbalanced data set. This would affect the accuracy score in the model building step if untreated. 

To treat this unbalance, we would use the `over sampling` 

```{r}
set.seed(42)
minority <- nrow(data_train[data_train$TARGET_FLAG == 1,])
majority <- nrow(data_train[data_train$TARGET_FLAG == 0,])
diff <- majority - minority
minority_index <- data_train[data_train$TARGET_FLAG == 1,]$INDEX
over_sample_train <- data.frame(INDEX = sample(minority_index, diff, TRUE)) %>%
  merge(data_train, .) %>%
  bind_rows(data_train)

data_train_balanced <- over_sample_train
```

check the balance again

```{r}
table(over_sample_train$TARGET_FLAG)
```


---

### Influential Leverage Points


---

## Model Building - Logit Models {.tabset .tabset-fade .tabset-pills}

Our objective is to predict both `TARGET_FLAG` and `TARGET_AMT`. `TARGET_FLAG` is a discrete response variable and for that reason it should be modeled using logistic regression which determines the probability that an individual will be in an accident.

```{r}
# Initialize a df that will store the metrics of models
models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```


```{r}
# A function to extract the relevant metrics from the summary and confusion matrix
score_model <- function(id, model, data, output=FALSE) {
  if (output) print(summary(model))
  glm.probs <- predict(model, type="response")
  # Confirm the 0.5 threshold
  glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
  results <- tibble(target=data$TARGET_FLAG, pred=glm.pred)
  results <- results %>%
    mutate(pred.class = as.factor(pred), target.class = as.factor(target))
  
  if (output) print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
  
  acc <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$overall['Accuracy']
  sens <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Sensitivity']
  spec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Specificity']
  #prec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Precision']
  res.deviance <- model$deviance
  null.deviance <- model$null.deviance  
  aic <- model$aic
  metrics <- list(res.deviance=res.deviance, null.deviance=null.deviance,aic=aic, accuracy=acc, sensitivity=sens, specificity=spec)
  metrics <- lapply(metrics, round, 3)
  
  if (output) plot(roc(results$target.class,glm.probs), print.auc = TRUE)
  model.df <- tibble(id=id, res.deviance=metrics$res.deviance, null.deviance=metrics$null.deviance, 
                         aic=metrics$aic, accuracy=metrics$accuracy, sensitivity=metrics$sensitivity, specificity=metrics$specificity)
  model.list <- list(model=glm.fit, df_info=model.df)
  return(model.list)
}
```

### Model 1 A&B: Logit Models

We construct null, full and reduced models. The null model contains only the intercept and forms the lower bound of complexity. The full model contains all predictors and is the upper bound, max complexity model. The reduced model is obtained by iteratively stepping through the predictors between the aforementioned bounds until only statistically significant predictors remain.
```{r}
mod1data <- data_train %>% dplyr::select(-c('TARGET_AMT','INDEX'))
#mod1data <- data_train_balanced %>% select(-c('TARGET_AMT','INDEX'))

model.null <- glm(TARGET_FLAG ~ 1,
                 data=mod1data,
                 family = binomial(link="logit")
                 )

model.full <- glm(TARGET_FLAG ~ .,
                 data=mod1data,
                 family = binomial(link="logit")
                 )
    
model.reduced <- step(model.null,
              scope = list(upper=model.full),
             direction="both",
             test="Chisq",
             trace=0,
             data=mod1data)

m1a <- score_model('model.full', model.full, mod1data, output = TRUE)
m1a$df_info
models.df <- rbind(models.df,m1a$df_info)
```

The summary output of the reduced model retains a number of statistically significant predictors. 

```{r}
m1b <- score_model('model.reduced', model.reduced, mod1data, output = TRUE)
m1b$df_info
models.df <- rbind(models.df,m1b$df_info)
```


```{r}
#summary(model.reduced)
```

We compute McFadden's pseudo R squared for logistic regression and we see that the difference between the full model and the reduced model is only marginal. 

```{r}
paste0('Full model = ',round(1-logLik(model.full)/logLik(model.null),4))
paste0('Reduced model = ',round(1-logLik(model.reduced)/logLik(model.null),4))
```

#### Diagnotics

We diagnose the reduced model for irregularities and violations of assumptions. The resulting logit values for the continuous predictors look mostly linear. 

```{r}
library(broom)
# Select only numeric predictors and predict
numdata <- mod1data %>%
  dplyr::select_if(is.numeric)
predictors <- colnames(numdata)
probabilities <- predict(model.reduced, type = "response")
# Bind the logit and tidying the data for plot
numdata <- numdata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(numdata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

#### Influential Values

Inspection the influential values reveals 3 observations of interest, all which have higher degrees and high bluebook values, except for sports car and a lower bluebook value.

```{r}
plot(model.reduced, which = 4, id.n = 3)

model.data <- augment(model.reduced) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)
```

#### Outliers

Only one outlier is identified which we will consider removing in a subsequent model.

```{r}
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw()

model.data %>% 
  filter(abs(.std.resid) > 3)
```

#### Multicollinearity

Looking into the multicollinearity, we see that the categorical variables `JOB` and `EDUCATION` are values greater than 5. These are categorical and therefore add degrees of freedom. We will remove them to compare the results but also consider penalized models to deal with multicollinearity.

```{r}
car::vif(model.reduced)
```

### Model 1 C&D: Balanced Data

As a comparison, we make use of the balance dataset assembled earlier. We compute both the full and reduced models and store the metrics in the summary dataframe. 

```{r}
mod1data_bal <- data_train_balanced %>% dplyr::select(-c('TARGET_AMT','INDEX'))

modelbal.null <- glm(TARGET_FLAG ~ 1,
                 data=mod1data_bal,
                 family = binomial(link="logit")
                 )

modelbal.full <- glm(TARGET_FLAG ~ .,
                 data=mod1data_bal,
                 family=binomial(link="logit")
                 )
    
modelbal.reduced <- step(modelbal.null,
              scope=list(upper=modelbal.full),
             direction="both",
             test="Chisq",
             trace=0,
             data=mod1data_bal)

m1c <- score_model('modelbal.full', modelbal.full, mod1data_bal, output = FALSE)
models.df <- rbind(models.df,m1c$df_info)
m1d <- score_model('modelbal.reduced', modelbal.reduced, mod1data_bal, output = FALSE)
models.df <- rbind(models.df,m1d$df_info)
```

### Model 1 E&F: Influential and Outlying Observations Removed

Here we model using the previous methods but discard the influential observations as well as the outlier. 

```{r}
mod1data_rmv <- data_train %>% filter(!INDEX %in% c(3722, 3592, 4690, 4742)) %>% dplyr::select(-c('TARGET_AMT','INDEX')) 

modelrmv.null <- glm(TARGET_FLAG ~ 1,
                 data=mod1data_rmv,
                 family = binomial(link="logit")
                 )

modelrmv.full <- glm(TARGET_FLAG ~ .,
                 data=mod1data_rmv,
                 family=binomial(link="logit")
                 )
    
modelrmv.reduced <- step(modelrmv.null,
              scope=list(upper=modelrmv.full),
             direction="both",
             test="Chisq",
             trace=0,
             data=mod1data_rmv)

m1e <- score_model('modelrmv.full', modelrmv.full, mod1data_rmv, output = FALSE)
models.df <- rbind(models.df,m1e$df_info)
m1f <- score_model('modelrmv.reduced', modelrmv.reduced, mod1data_rmv, output = FALSE)
models.df <- rbind(models.df,m1f$df_info)
```

### Model 1: Summary

Finally we consider the results of these models. The models are of fairly comparable performance. The models using the balanced data lose in accuracy and specificity but gain in sensitivity For the other models, the metrics are comparable. The model `modelrmv.reduced` has the smaller AIC overall but the full version actually has a slightly higher R squared. We proceed with the smaller model with hopes of reducing variances at the expense of introducing a bit a bias.

```{r}
models.df

paste0('Full model  = ',round(1-logLik(model.full)/logLik(model.null),4))
paste0('Reduced model  = ',round(1-logLik(model.reduced)/logLik(model.null),4))

paste0('Full model (bal) = ',round(1-logLik(modelbal.full)/logLik(modelbal.null),4))
paste0('Reduced model (bal) = ',round(1-logLik(modelrmv.reduced)/logLik(modelrmv.null),4))

paste0('Full model (rmv) = ',round(1-logLik(modelbal.full)/logLik(modelbal.null),4))
paste0('Reduced model (rmv) = ',round(1-logLik(modelrmv.reduced)/logLik(modelrmv.null),4))
```

### Model 2A: Penalized Logistic Model

Since the basic model contained many predictor variables, we take a look at a penalized logistic regression model which imposes a penalty on the model for having too many predictors. We will indentify the best shrinkage factor `lambda` through cross validation with an 80%/20% train/test data partitioning. 

We fit a lasso regression model (alpha=1) and plot the cross-validation error over the log of lambda. The number of predictors are shown on top and the vertical lines represent the optimal (minimal) value of lambda as well as the value of lambda which minimizes the number of predictors but still remains within 1 standard error of the optimal. We will consider models with both of these lamdba values.

```{r}
# Prepare the data
library(glmnet)

mod2_data <- data_train %>% dplyr::select(-c('TARGET_AMT','INDEX'))

# Split the data into training and test set
set.seed(42)
training.samples <- mod2_data$TARGET_FLAG %>% createDataPartition(p = 0.8, list = FALSE)
train.data <- mod2_data[training.samples, ]
test.data <- mod2_data[-training.samples, ]
# Dummy code categorical predictor variables
x <- model.matrix(TARGET_FLAG~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$TARGET_FLAG

# Find the best lambda using cross-validation
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
```


```{r}
paste0('lambda.min = ',cv.lasso$lambda.min)
paste0('lambda.1se = ',cv.lasso$lambda.1se) # simplest model but also lies within one standard error of the optimal value of lambda
```

The columns below compare the variables that are dropped in the lasso regression for the optimal and smallest model lambdas.

```{r}
# Display regression coefficients
c_min_lambda <- coef(cv.lasso, cv.lasso$lambda.min)
c_1se_lambda <- coef(cv.lasso, cv.lasso$lambda.1se)
cbind(c_min_lambda, c_1se_lambda)
```

#### Predicting TARGET_FLAG

When comparing the accurancy of the two lasso penalized models, we see that the difference in accuracy is marginal and slightly lower that the simple logit models. For this reason, our selected best model will be the reduced model with influential observations and outliers removed.

```{r}
# Final model with lambda.min
lasso.model.min <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(TARGET_FLAG ~., test.data)[,-1]
probabilities <- lasso.model.min %>% predict(newx = x.test)
predicted.classes <- as.factor(ifelse(probabilities > 0.5, 1, 0))
# Model accuracy rate
observed.classes <- as.factor(test.data$TARGET_FLAG)
mean(predicted.classes == observed.classes)
print(confusionMatrix(predicted.classes,observed.classes, positive = "1"))
```

```{r}
# Final model with lambda.1se
lasso.model.se1 <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)
# Make prediction on test data
x.test <- model.matrix(TARGET_FLAG ~., test.data)[,-1]
probabilities <- lasso.model.se1 %>% predict(newx = x.test)
predicted.classes <- as.factor(ifelse(probabilities > 0.5, 1, 0))
# Model accuracy rate
observed.classes <- as.factor(test.data$TARGET_FLAG)
#mean(predicted.classes == observed.classes)
print(confusionMatrix(predicted.classes,observed.classes, positive = "1"))
```

## Model Building - Mutiple Regression Models {.tabset .tabset-fade .tabset-pills}

#### Predicting TARGET_AMT

We subset the dataset to pick out only the observations which were involved in accidents. This is also the population that has previously filed claims, so this information will be used to predict the `TARGET_AMT`.

### Model 3 A&B: Multiple Regression

The simple multiple regression model delivers a poor performance with a very low R-squared value. For this reason, we consider the next model using weights as a comparison. 

```{r}
data_train2 <- data_train %>% dplyr::select(c(-'INDEX')) %>% filter(TARGET_FLAG==1) %>% dplyr::select(-c('TARGET_FLAG'))
mod3a <- lm(TARGET_AMT ~ ., data_train2)
summary(mod3a)
```

We build the reduced model as before and output the summary. The Adjusted R-squared remains very low at 1.8%. The diagnostic QQ plot reveals a large deviation from normal in the upper quantiles that heavily affects the results.  

```{r}
modeltgt.null <- lm(TARGET_AMT ~ 1, data=data_train2)
modeltgt.full <- lm(TARGET_AMT ~ ., data=data_train2)
modeltgt.reduced <- step(modeltgt.null,
              scope=list(upper=modeltgt.full),
             direction="both",
             trace=0,
             data=data_train2)

summary(modeltgt.reduced)
plot(modeltgt.reduced)
```

### Model 4: Weighted Least Squares

We explore a weighted least squares model hoping for a better performance and manage to get a bump up 6.8%. We see that compared to the previous model the upper range of the QQ plot rapidly increases.

```{r}
lm4 <- lm(TARGET_AMT ~. -INDEX -TARGET_FLAG, data = data_train)
summary(lm4)
sd <- 1 / lm(abs(lm4$residuals)~lm4$fitted.values)$fitted.values^2
lm4_wls <- lm(TARGET_AMT ~. -INDEX -TARGET_FLAG, data = data_train, weights = sd)

lm4_wls_step <- stepAIC(lm4_wls, method = "leapBackward", trace = FALSE)
summary(lm4_wls_step)
plot(lm4_wls_step)
```


## Model Selection 

We use the final selected models to make the predictions and write the results to file. To predict `TARGET_FLAG` we use `modelrmv.reduced` and the WLS model for `TARGET_AMT`. Once the individuals who are predicted to have accidents are identified, we filter them and predict the amount of that subset. 

```{r}
eval_probs <- predict(modelrmv.reduced, newdata=data_test %>% dplyr::select(-INDEX, -TARGET_AMT), type='response')
data_test$TARGET_FLAG <- ifelse(eval_probs > 0.5, 1, 0)
data_test$TARGET_AMT <- 0.0
data_test[data_test$TARGET_FLAG == 1,]$TARGET_AMT <- predict(lm4_wls_step, newdata=data_test %>% filter(TARGET_FLAG==1) %>% dplyr::select(-c('TARGET_FLAG','INDEX')))

data_test

write.csv(data_test, "data_test_results.csv")
```

