---
title: "Data 621 - Homework 5"
author: "Dhairav Chhatbar, Mael Illien, Salma Elshahawy"
date: "11/23/2020"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
# Data 621 Homework5 

## Introduction 

In this assignment, we are tasked with predicting the number of cases of wine sold to wine distribution companies following a sampling. The target variable, cases of wine sold, is count data and therefore will be modeled using appropriate techniques such as Poisson and Negative Binomial regressions. 

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(skimr)
library(ggcorrplot)
library(tidyverse)
library(PerformanceAnalytics)
library(DMwR)
library(caret)
library(kableExtra)
library(DescTools)
library(cowplot)
library(pROC)
library(broom)
library(car)
library(pscl)
```

```{r message=FALSE, warning=FALSE}
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_5/wine-training-data.csv", header = TRUE)
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_5/wine-evaluation-data.csv", header = TRUE)
```

## Data Exploration {.tabset .tabset-fade .tabset-pills}

### Data Exploration

All the variable in this dataset are numeric and continuous except for `AcidIndex`, `STARS` and `LabelAppeal` which are discrete. The target variable `TARGET` is also discrete. There are a number of missing observations for certain chemical compositiion variables as well as a large number of wines with no `STARS` rating. The distribution of the continous variables appear well centered.


```{r data_summary_train, message=FALSE, warning=FALSE}
skim(data_train)
```

```{r}
skim(data_test)
```


```{r}
# remove index column as it is not needed
data_train <- data_train %>% 
  dplyr::select(-"INDEX")
data_test <- data_test %>% 
  dplyr::select(-"IN")
```

### Visualization

The visualizations below display the distributions of the dataset. All continuous variables are centered and close to normally distributed. We note that some variables are centered around zero and take negative values which is unexpected and will be investigated further and transformed for our analysis. The distribution of the `TARGET` variable looks like it could be well described by the poisson distribution which has equal mean and variance but the high number of zero values justifies the use of a zero-inflated model as well. The mean and variance of `TARGET` are 3.02 and 3.71 respectively, which is close enough to satisfy the equal mean-variance assumption of the poisson distribution.


```{r fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
# histogram
data_train %>% 
  dplyr::select(-c("AcidIndex", "STARS", "TARGET", "LabelAppeal")) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scale = "free",  ncol = 3) +
  geom_histogram(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)), fill="pink") +
  theme_minimal()
```

```{r variables_distribution, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
data_train %>% 
  ggplot(aes(x=TARGET)) + 
  geom_histogram(fill='pink')
```

In the box plot below, we plot several variables in to two panels. TotalSulfurDioxide, FreeSulfurDioxide, and ResidualSugar variables have large ranges compared to other variables. Therefore, we separated those variables in to a different panel to view their distribution. From both panel, we can tell a high number of variables have numerous outliers. 

```{r fig.height=8, fig.width=10}
library(ggpubr)
# boxplot
p1 <- data_train %>% 
  dplyr::select(-c("TotalSulfurDioxide", "FreeSulfurDioxide", "ResidualSugar")) %>% 
  gather(na.rm = TRUE) %>% 
  ggplot(aes(factor(key), value)) +
  geom_boxplot(outlier.colour = "#e281cf", outlier.shape = 1,  color = "#5aa1ed") +
  coord_flip() +
  labs(title = "Boxplot of Chemical Properties of Wine", x = "Chemical Properties", y = "Values") +
  theme_minimal()
p2 <- data_train %>% 
  dplyr::select(c("TotalSulfurDioxide", "FreeSulfurDioxide", "ResidualSugar")) %>% 
  gather(na.rm = TRUE) %>% 
  ggplot(aes(factor(key), value)) +
  geom_boxplot(outlier.colour = "#e281cf", outlier.shape = 1, color = "#5aa1ed") +
  #labs(title = "Boxplot of Chemical Properties of Wine", x = "Chemical Properties", y = "Values") +
  theme_minimal()
ggarrange(p1, p2)
```


In the bar char below, `AcidIndex` tells us large quantity of wine were sold with the index number `7` and `8`. `LabelAppeal` tells us generic labeled wine sells the most; however, better label does yield higher number of wine samples per order. Lastly, `STARS` tells us excellent quality does not result in high wine orders. It could be due to high star wine bottle’s high price tag.

```{r fig.height=10, fig.width=8}
# barchart
p3 <- data_train %>% 
  dplyr::select(TARGET, STARS) %>% 
  mutate(STARS = as.factor(STARS),
         TARGET = as.factor(TARGET)) %>% 
  ggplot(aes(STARS)) +
  geom_bar(aes(fill = TARGET)) +
  theme_minimal()
p4 <- data_train %>%
  dplyr::select(TARGET, LabelAppeal) %>% 
  mutate(STARS = as.factor(LabelAppeal),
         TARGET = as.factor(TARGET)) %>% 
  ggplot(aes(LabelAppeal)) +
  geom_bar(aes(fill = TARGET)) +
  theme_minimal()
p5 <- data_train %>% 
  dplyr::select(TARGET, AcidIndex) %>% 
  mutate(STARS = as.factor(AcidIndex),
         TARGET = as.factor(TARGET)) %>% 
  ggplot(aes(AcidIndex)) +
  geom_bar(aes(fill = TARGET)) +
  theme_minimal()
ggarrange(p5, ggarrange(p3, p4, ncol = 2, nrow = 1, legend = "none"), nrow = 2, common.legend = TRUE)
```

#### Correlations with Response Variable

```{r}
# top correlation
wine_train_corr <- data_train %>% 
  drop_na() %>% 
  cor()
kable(sort(wine_train_corr[,1], decreasing = T), col.names = c("Correlation")) %>% 
  kable_styling(full_width = F)
```

In the correlation table and plot below, we see, `STARS` and `LabelAppeal` are most positively correlated variables with the response variable. We expected this because our variable description mentions these variable’s theoretical affect are higher than other variables. Also, we some mild negative correlation between the response variable and `AcidIndex` variable.

```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
library(corrplot)
library(RColorBrewer)
# correlation plot
corrplot(wine_train_corr, 
         method = "number", 
         type = "lower",
         col = brewer.pal(n = 15, name = "Reds"),
         number.cex = .7, tl.cex = .7,
         tl.col = "black", tl.srt = 45)
```

## Data Preparation {.tabset .tabset-fade .tabset-pills}

Using the `aggr` function from `VIM package`, We see several variables have missing values. According to UCI Machine Learning, who published this dataset, all wine contain some natural sulfites. Therefore, we will impute the missing values for sulfite chemical properties. Also, it’s rare to find wines with less than 1 gram/liter of sugar. We will impute this as well. Matter of fact, since all missing values are missing at random, we will impute all the missing values using the mice package and random forest method. Mice uses multivariate imputations to estimate the missing values. Using multiple imputations helps in resolving the uncertainty for the missingness. Our target variable will be removed as a predictor variable but still will be imputed. Our response variables will be removed as predictor variables but still will be imputed.

```{r fig.height=10, fig.width=10, message=FALSE}
library(VIM)
# missing value columns
aggr(data_train, 
     sortVars=TRUE, 
     labels=names(data_train), 
     cex.axis=.5, 
     bars = FALSE, 
     col = c("white", "#E46726"),
     combined = TRUE,
     #border = NA,
     ylab = "Missing Values") 
```

```{r message=FALSE, warning=FALSE}
library(mice)
# imputating train data
init <- mice(data_train)
meth <- init$method
predM <- init$predictorMatrix
predM[, c("TARGET")] <- 0 #this code will remove the variable as a predictor but still will be imputed
data_train_impute <- mice(data_train, method = 'rf', predictorMatrix=predM)
data_train_imputed <- complete(data_train_impute)
print(paste0("Missing value after imputation: ", sum(is.na(data_train_imputed))))
```

## Model Building {.tabset .tabset-fade .tabset-pills}

### Model 1: Poisson (Raw data) 

```{r}
# poisson model with the missing values
model1 <- glm(TARGET ~ ., family = poisson, data_train)
summary(model1)
print('Goodness of Fit Test:')
with(model1, cbind(res.deviance = deviance, df = df.residual,  p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

The deviance residuals is quite symmetrical. This means that the predicted points are close to actual observed points. As expected and seen in our correlation table, `STARS`, `LabelAppeal` and `AcidIndex` are significant variables. And the variation in standard error is low. The goodness of fit test has a high p value which indicates that the model fits the data well.


###  Model 2: Poisson (Imputed Data)

```{r}
# poisson model with the imputed values
model2 <- glm(TARGET ~ ., family = poisson, data_train_imputed)
summary(model2)
print('Goodness of Fit Test:')
with(model2, cbind(res.deviance = deviance, df = df.residual, p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

Again the deviance residuals are quite the same. However, with imputation, there are more significant variables introduced in the model. Additionally, the AIC score improved dramatically from 23172 to 50384. Also, the deviance residuals decreased significantly to 18,412 from 40,000. However, the goodness of fit test  has a very small p value, suggesting that this model does not fit the data well. Since the residual deviance is greater than the degrees of freedom, then some over-dispersion exists.

Given what we observed when looking at the distribution of `TARGET`, we should expext the inflated count of zeros to affect the model and bias results. For this reason, we move to a zero-inflated model to reflect this.

### Model 3: Quasipoisson Model

We try a quasipoisson model to account for nay overdispersion and to see if the results change significantly. As seen in the summary below, the models are nearly identical.

```{r}
# poisson model with the imputed values
model3 <- glm(TARGET ~ ., family = quasipoisson(link='log'), data_train_imputed)
summary(model3)
print('Goodness of Fit Test:')
with(model3, cbind(res.deviance = deviance, df = df.residual, p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```



### Model 4: Zero Inflated

We saw earlier that the dependent variable had an excess number of zeros which skewed the distribution from a typical poisson. The zero inflated model generates coefficients for the zero count part of the model as well as for the count part. 

```{r}
model4 <- zeroinfl(TARGET ~., data_train_imputed, dist = 'poisson')
summary(model4)
```

```{r}
model4b <- zeroinfl(TARGET ~ . - FixedAcidity - Density, data_train_imputed, dist = 'poisson')
summary(model4b)
```

## Model Selection 
Based on the models tested, we select the Zero Inflated model due to the highest accuracy of the 4 models
```{r, fig.width=10, fig.height=5}
pred_train <- data.frame(TARGET=data_train_imputed$TARGET, 
                         model2=model2$fitted.values, 
                         model3=model3$fitted.values, 
                         model4b=model4b$fitted.values)
pred_train <- round(pred_train, 0)
colnames(pred_train) <- c("TARGET","Poisson (Imputed)" ,"Quasipoisson Model", "Zero Inflated")

pred_train %>%
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scale = "free",  ncol = 4) +
  geom_bar(fill="pink") +
  theme_minimal() + labs(x="Cases Bought", y = "Count", title = "Prediction Histogram")


```

```{r}

model2_fitted.values <- factor(round(model2$fitted.values),levels=rev(0:9))
model3_fitted.values <- factor(round(model3$fitted.values),levels=rev(0:9))
model4b_fitted.values <- factor(round(model4b$fitted.values),levels=rev(0:9))


m2_cfm <- confusionMatrix(model2_fitted.values, factor(data_train_imputed$TARGET,levels=rev(0:9)))
m3_cfm <- confusionMatrix(model3_fitted.values, factor(data_train_imputed$TARGET,levels=rev(0:9)))
m4_cfm <- confusionMatrix(model4b_fitted.values, factor(data_train_imputed$TARGET,levels=rev(0:9)))

models_sum <- data.frame(m2_cfm$overall, m3_cfm$overall, m4_cfm$overall)
colnames(models_sum) <- c("Poisson (Imputed)" ,"Quasipoisson Model", "Zero Inflated")
round(models_sum, 2)

```



```{r, message=FALSE, warning=FALSE}
test_predict <- predict(model4b, newdata=data_test)


data_test$TARGET <- test_predict
ggplot(data_test, aes(x=round(data_test$TARGET, 0))) + geom_bar(fill="orchid3") + theme_minimal()+
  labs(y="Count", title = "Prediction: Zero Inflated Model (Model4b)") + scale_x_discrete(name = "Cases Bought", 
                    limits=c("1","2","3","4", "5", "6", "7", "8"))
  
write.csv(test_predict, "WinePredictions.csv")
```