---
title: "Data 621 - Assignment_1"
author: "Salma Elshahawy"
date: "9/9/2020"
output: html_document
---

## Setup

```{r, message=FALSE, warning=FALSE}
library(skimr)
library(ggplot2)
library(ggcorrplot)
library(dplyr)
library(tidyr)
library(PerformanceAnalytics)
```

```{r}
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_1/datasets/moneyball-evaluation-data.csv", header = TRUE) %>%select(-INDEX)
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_1/datasets/moneyball-training-data.csv", header = TRUE) %>% select(-INDEX)
```

```{r}
# MI: Preserving original dataset for downstream analysis. Harmonize data_train later. 
data_train_mi <- data_train 
```


## Data Exploration

```{r}
dim(data_train)
```

```{r}
# list types for each attribute
sapply(data_train, class)
```

```{r}
skim(data_train)
```

```{r message=FALSE, warning=FALSE}
library(kableExtra)
data_train %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(data_train) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable() %>%
  kable_styling()
```

The hit by pitcher variable is missing over 90% of it’s data. We will exclude it from consideration in our model.

Caught stealling a base (TEAM_BASERUN_CS) is next on the list. It may be possible to predict it using TEAM_BASERUN_SB since they are strongly correlated, but there are 131 times they both are missing data.

MI: TRY
- Drop variable if unlikely event in real life (HBP: hit by pitcher and CS: caught stealing base are rare)
- Impute (try: mean, 0, drop)


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
#missing_per <- as.data.frame(t(data_train %>% select(everything()) %>% #summarise_all(funs(sum(is.na(.))))/nrow(data_train))) %>% arrange(desc(V1))
#missing_per
data_train %>% 
  gather() %>% 
  ggplot( aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

Let's take a closer look at the `TEAM_BASERUN_SB` 

```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  ggplot(aes(TEAM_BASERUN_SB)) + 
  geom_histogram(bins = 50, fill = 'pink') +
  geom_vline(aes(xintercept = mean(TEAM_BASERUN_SB, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BASERUN_SB, na.rm = T)), col = "blue", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Stolen Bases",
       caption = "* Red line is the mean value and blue is the median") + 
  theme_classic()
```

```{r fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(data_train, aes(x=TARGET_WINS)) + 
    geom_histogram(aes(y=..density..),binwidth = 3, colour="black", fill="red") +
    geom_density(alpha=.8, fill="pink") + 
  theme_classic()
```


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
q <- cor(data_train)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 
```

Correlations with Response Variable
Let’s take a look at how the predictors are correlated with the response variable:
```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "pink", color="pink") + 
  geom_smooth(method = "lm", se = FALSE, color = "red") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

It looks like the `TEAM_PITCHING_BB`, `TEAM_BATTING_H` , `TEAM_BATTING_BB`, `TEAM_BATTING_2B`and `TEAM_BATTING_HR` have the strongest correlations with wins. None of these correlations are particularly strong. This suggests there is a lot of ‘noise’ in these relationships due to outliers.

MI: 
- TEAM_PITCHING_HR (homeruns pitched) is slightly correlated win the number of wins. This is clearly unexpected because pitched homeruns are a negative for the pitching team. 
- TEAM_PITCHING_SO (strikeouts by pitcher) likely suffers from outliers because we'd expect strikeouts to be a positive thing for the pitching team. There is a large influential point.
- TEAM_PITCHING_BB (walks allowed by pitcher) might suffer from outliers. We'd expected a negative correlation since a walk allowed is a negative for the pitching team.

## Data Preparation

MI: this passage is just for reference when dealing with leverage point. Can be removed later
Outliers & Leverage Points

In summary, an outlier is a point whose standardized residual falls outside the interval from –2 to 2. Recall that a bad leverage point is a leverage point which is also an outlier. Thus, a bad leverage point is a leverage point whose standar- dized residual falls outside the interval from –2 to 2. On the other hand, a good leverage point is a leverage point whose standardized residual falls inside the interval from –2 to 2.

Recall that the rule for simple linear regression for classifying a point as a leverage point is hii > 4/n . 

## Fixing zeros and missing values
remove invalid data, drop HIT_BY_PITCHER, finally impute using knn

```{r}
data_train <- data_train %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO)) %>%
  mutate(TEAM_PITCHING_SO = ifelse(TEAM_PITCHING_SO > 5346, NA, TEAM_PITCHING_SO)) %>%
  select(-TEAM_BATTING_HBP)
```

```{r}
set.seed(42)
library(DMwR)
knn <- data_train %>% knnImputation()
imputer <- is.na(data_train$TEAM_BATTING_SO)
data_train[imputer,"TEAM_BATTING_SO"] <- knn[imputer,"TEAM_BATTING_SO"] 
imputer <- is.na(data_train$TEAM_BASERUN_SB)
data_train[imputer,"TEAM_BASERUN_SB"] <- knn[imputer,"TEAM_BASERUN_SB"] 
imputer <- is.na(data_train$TEAM_BASERUN_CS)
data_train[imputer,"TEAM_BASERUN_CS"] <- knn[imputer,"TEAM_BASERUN_CS"] 
imputer <- is.na(data_train$TEAM_PITCHING_SO)
data_train[imputer,"TEAM_PITCHING_SO"] <- knn[imputer,"TEAM_PITCHING_SO"]
imputer <- is.na(data_train$TEAM_FIELDING_DP)
data_train[imputer,"TEAM_FIELDING_DP"] <- knn[imputer,"TEAM_FIELDING_DP"]
```

## Feature engineering ( add the missing feature )

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
# New variable: TEAM_BATTING_1B
add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}
data_train <- add_features(data_train)
data_test <- add_features(data_test)
```

#### Feature engineering (continued)

MI: I am creating a new df to not influence the models downstream that may select all features. However, I think all engineered featured should be included in the same dataset, and the individual models under test should select the variables, either from the original set or from the engineered or both. 

Research into baseball statistics suggests the use of the following engineered variables which are composites of variables from the base dataset. These variables, namely "at bats", "batting average", "on base percentage" and "slugging percentage' provide more insight into a team's batting performance by providing variables quantifying the number of opportunities of hitting the ball, the number of times the ball was actually hit, and when hit, how many bases the batter was able to reach. All these variables are representations of a team's ability to score points. (Maybe discuss variables that we expect to benefit the opposing team).

```{r}
data_train_mi <- add_features(data_train)

# Creating "at bats" variable representing every time a batter steps up to bat
data_train_mi <- data_train_mi %>% mutate(TEAM_BATTING_AB = TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP + TEAM_BATTING_SO)
# Creating "batting average" variable
data_train_mi <- data_train_mi %>% mutate(TEAM_BATTING_AVG = TEAM_BATTING_H/TEAM_BATTING_AB)
# Creating "on base percentage" representing the proportion of ways to get a base out of total opportunities to hit the ball
data_train_mi <- data_train_mi %>% mutate(TEAM_BATTING_OBP = (TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP)/(TEAM_BATTING_AB + TEAM_BATTING_BB + TEAM_BATTING_HBP))
# Creating "slugging percentage" which is a weighted sum of hits by number of bases acquired divided by opportunities to hit the ball
data_train_mi <- data_train_mi %>% mutate(TEAM_BATTING_SLG = (TEAM_BATTING_1B + 2*TEAM_BATTING_2B + 3*TEAM_BATTING_3B + 3*TEAM_BATTING_HR)/TEAM_BATTING_AB)
```

Encapsulating the at above into a function. (Use only one of these in final report). Function makes more sense for re-use in prediction

```{r}
add_advanced_bb_features <- function(df) {
  df %>% 
    mutate(TEAM_BATTING_AB = TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP + TEAM_BATTING_SO) %>%
    mutate(TEAM_BATTING_AVG = TEAM_BATTING_H/TEAM_BATTING_AB) %>%
    mutate(TEAM_BATTING_OBP = (TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP)/(TEAM_BATTING_AB + TEAM_BATTING_BB + TEAM_BATTING_HBP)) %>%
    mutate(TEAM_BATTING_SLG = (TEAM_BATTING_1B + 2*TEAM_BATTING_2B + 3*TEAM_BATTING_3B + 3*TEAM_BATTING_HR)/TEAM_BATTING_AB)
}
```


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  gather() %>% 
  ggplot( aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r}
quick_summary <- function(df){
  df %>%
    summary() %>%
    kable() %>%
    kable_styling()
}
quick_summary(data_train)
```

### Models

split the data into test and train set with 0.7 t 0.3 ratio using caret package

```{r}
library(caret)
train_index <- createDataPartition(data_train$TARGET_WINS, p = .7, list = FALSE, times = 1)
moneyball_train <- data_train[train_index,]
moneyball_test <- data_train[-train_index,]
```

### Model 1 - Backwards Elimination


### Model 2 - Engineered Variables

MI: Too many values are removed. These engineered features rely on TEAM_BATTING_HBP in the calculations but this is the variable with the most missing data. Treatment of this variable will affect downstream results.

```{r}
train_mi %>% 
  select(TEAM_BATTING_AB, TEAM_BATTING_AVG, TEAM_BATTING_OBP, TEAM_BATTING_SLG) %>%
  gather() %>% 
  ggplot(aes(x=value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r}
data_train_mi %>%
  gather(variable, value, -c(TARGET_WINS:TEAM_BATTING_1B)) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "pink", color="pink") + 
  geom_smooth(method = "lm", se = FALSE, color = "red") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```


```{r}
mi_m1 <- lm(TARGET_WINS ~ TEAM_BATTING_AB + TEAM_BATTING_AVG + TEAM_BATTING_OBP + TEAM_BATTING_SLG, data = data_train_mi)
summary(mi_m1)
```

The standardized residual plots show quite a few points outside the -2,2 range, which might justify removing those observations.

Residuals vs Fitted: while the line is not quite horizontal, the constant variance assumption seems met
Normal Q-Q plot: normality assumption is met
Root(Squared Residuals) vs Fitted Values:
Residuals vs Leverage: a few points have standardized residuals outside the (-2,2) ranhe which might justify removing those observations.

```{r}
plot(mi_m1)
```

MI: tyring out predictions on this model

```{r}
data_test_mi <- add_features(data_test)
data_test_mi <- add_advanced_bb_features(data_test_mi)

pred.w.plim <- predict(mi_m1, data_test_mi, interval = "prediction")
pred.w.clim <- predict(mi_m1, data_test_mi, interval = "confidence")
```



### Model 3 - Transformations

## Model Selection

## Prediction

