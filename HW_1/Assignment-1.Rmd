---
title: "Data 621 - Assignment 1"
author: "Dhairav Chhatbar"
date: "9/9/2020"
output: html_document
---

```{r, message=FALSE, warning=FALSE}
library(skimr)
library(ggplot2)
library(ggcorrplot)
library(dplyr)
library(tidyr)
library(PerformanceAnalytics)
```


```{r}
data_test <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_1/datasets/moneyball-evaluation-data.csv", header = TRUE)
data_train <- read.csv("https://raw.githubusercontent.com/salma71/Data_621/master/HW_1/datasets/moneyball-training-data.csv", header = TRUE)
```

## Data Exploration

## Data Preparation

1. Impute missing values
2. Combine/new variables
3. Transforms

Outliers & Leverage Points

In summary, an outlier is a point whose standardized residual falls outside the interval from –2 to 2. Recall that a bad leverage point is a leverage point which is also an outlier. Thus, a bad leverage point is a leverage point whose standar- dized residual falls outside the interval from –2 to 2. On the other hand, a good leverage point is a leverage point whose standardized residual falls inside the interval from –2 to 2.

Recall that the rule for simple linear regression for classifying a point as a leverage point is hii > 4/n . 

## Build Models
## Select Models

```{r}
dim(data_train)
```

```{r}
# list types for each attribute
sapply(data_train, class)
```

```{r}
skim(data_train)
```

```{r message=FALSE, warning=FALSE}
library(kableExtra)
data_train %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(data_train) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable() %>%
  kable_styling()
```

The hit by pitcher variable is missing over 90% of it’s data. We will exclude it from consideration in our model.

Caught stealling a base (TEAM_BASERUN_CS) is next on the list. It may be possible to predict it using TEAM_BASERUN_SB since they are strongly correlated, but there are 131 times they both are missing data.

MI: TRY
- Drop variable if unlikely event in real life (HBP: hit by pitcher and CS: caught stealing base are rare)
- Impute (try: mean, 0, drop)


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
#missing_per <- as.data.frame(t(data_train %>% select(everything()) %>% #summarise_all(funs(sum(is.na(.))))/nrow(data_train))) %>% arrange(desc(V1))
#missing_per

data_train %>% 
  select(-INDEX) %>% 
  gather() %>% 
  ggplot( aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

Let's take a closer look at the `TEAM_BASERUN_SB` 

```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  ggplot(aes(TEAM_BASERUN_SB)) + 
  geom_histogram(bins = 50, fill = 'pink') +
  geom_vline(aes(xintercept = mean(TEAM_BASERUN_SB, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BASERUN_SB, na.rm = T)), col = "blue", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Stolen Bases",
       caption = "* Red line is the mean value and blue is the median") + 
  theme_classic()
```

```{r fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
ggplot(data_train, aes(x=TARGET_WINS)) + 
    geom_histogram(aes(y=..density..),binwidth = 3, colour="black", fill="red") +
    geom_density(alpha=.8, fill="pink") + 
  theme_classic()
```


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
to_plot <- data_train %>% 
  dplyr::select(-INDEX)

q <- cor(to_plot)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 

```

Correlations with Response Variable
Let’s take a look at how the predictors are correlated with the response variable:
```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "pink", color="pink") + 
  geom_smooth(method = "lm", se = FALSE, color = "red") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

It looks like the `TEAM_PITCHING_BB`, `TEAM_BATTING_H` , `TEAM_BATTING_BB`, `TEAM_BATTING_2B`and `TEAM_BATTING_HR` have the strongest correlations with wins. None of these correlations are particularly strong. This suggests there is a lot of ‘noise’ in these relationships due to outliers.

MI: 
- TEAM_PITCHING_HR (homeruns pitched) is slightly correlated win the number of wins. This is clearly unexpected because pitched homeruns are a negative for the pitching team. 
- TEAM_PITCHING_SO (strikeouts by pitcher) likely suffers from outliers because we'd expect strikeouts to be a positive thing for the pitching team. There is a large influential point.
- TEAM_PITCHING_BB (walks allowed by pitcher) might suffer from outliers. We'd expected a negative correlation since a walk allowed is a negative for the pitching team.


### Data Prep

```{r}
data_train <- data_train %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO)) %>%
  mutate(TEAM_PITCHING_SO = ifelse(TEAM_PITCHING_SO > 5346, NA, TEAM_PITCHING_SO)) %>%
  select(-TEAM_BATTING_HBP)
```

```{r}
set.seed(42)
library(DMwR)
knn <- data_train %>% knnImputation()
impute_me <- is.na(data_train$TEAM_BATTING_SO)
data_train[impute_me,"TEAM_BATTING_SO"] <- knn[impute_me,"TEAM_BATTING_SO"] 
impute_me <- is.na(data_train$TEAM_BASERUN_SB)
data_train[impute_me,"TEAM_BASERUN_SB"] <- knn[impute_me,"TEAM_BASERUN_SB"] 
impute_me <- is.na(data_train$TEAM_BASERUN_CS)
data_train[impute_me,"TEAM_BASERUN_CS"] <- knn[impute_me,"TEAM_BASERUN_CS"] 
impute_me <- is.na(data_train$TEAM_PITCHING_SO)
data_train[impute_me,"TEAM_PITCHING_SO"] <- knn[impute_me,"TEAM_PITCHING_SO"]
impute_me <- is.na(data_train$TEAM_FIELDING_DP)
data_train[impute_me,"TEAM_FIELDING_DP"] <- knn[impute_me,"TEAM_FIELDING_DP"]
```

```{r}

```

MI:
- New variables: TEAM_BATTING_1B: TEAM_BATTING_H - SUM(TEAM_BATTING_i) where i = 2B,3B,HR
- Transforms

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
# New variable: TEAM_BATTING_1B
add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}

data_train <- add_features(data_train)
data_test <- add_features(data_test)
```


```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
data_train %>% 
  select(-INDEX) %>% 
  gather() %>% 
  ggplot( aes(x= value)) + 
  geom_density(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r}
quick_summary <- function(df){
  df %>%
    summary() %>%
    kable() %>%
    kable_styling()
}

quick_summary(data_train)
```



### Models

```{r}
library(caret)
train_index <- createDataPartition(data_train$TARGET_WINS, p = .7, list = FALSE, times = 1)
moneyball_train <- data_train[train_index,]
moneyball_test <- data_train[-train_index,]
```

```{r}
# simple model
m1 <- lm(TARGET_WINS ~., data = data_train)

summary(m1)
```

The full model Residuals vs Leverage plot shows that data point 1486 is beyond Cook's distance

```{r}
plot(m1)
```

```{r}
mi_m1 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_FIELDING_E, data = data_train)
summary(mi_m1)
```
```{r}
plot(mi_m1)
```

