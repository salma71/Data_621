---
title: "DATA 621 Final Project - Stock Predictions Methods"
author: |
  | **Dhairav Chhatbar, Mael Illien, Salma Elshahawy**
  | DATA 621, Master of Science in Data Science,
  | City University of New York
link-citations: yes
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
nocite: |
  @wickham_ggplot2_2009, @RCoreTeam, @Chang2015
bibliography: citations.bib
---


```{r options_pkgs, echo=F, warning=F, message=F, results=F}
# knitr::opts_chunk$set(error = F, message = F, # tidy = T,
#                       cache = T, warning = T, 
#                       results = 'hide', # suppress code output
#                       echo = F,         # suppress code
#                       fig.show = 'hide' # suppress plots
#                       )
library(readr)
library(skimr)
library(knitcitations)
library(pander)
library(tidyverse)
library(corrplot)
library(quantmod)
library(data.table)
library(car)
library(caret)
library(nlme)
```

# Abstract
Use 250 words or less to summarize your problem, methodology, and major outcomes.

Problem:
Methodology:
Major Outcomes:

There are a few ways of predicting stock prices. From blind guessing to machine learning, many techniques have been tested to try to achieve decent results. Capturing the complexity of the market in a model is a daunting task but it can be broken down in a number of appraoches. 

Since predicting the exact percent increase of a stock the next day is difficult, we can instead try instead to predict the direction of the movement. Alternatively, we can look at the trends and seasonality in the data and try to predict with some level of confidence where the price would like in an arbitrary number of days.

In this paper, we implemented models using logistic regression, panel regresssion, generalized least squares (maybe) and autoregressive models using ARIMA.

The logistic models which try to predict the direction of the next day's movement yielded poor results. Hardly any of the predictor variable were significant, and the classification accuracies were around 52% which is hardly better than guessing. On the other hand, the auto-regressive models that predict subsequent values of a stock time series were of greater use. This required finding a suitable combination of parameters to fit the data closely. 

The panel models




# Key Words
Select a few key words (up to five) related to your work.

1. Time series
2. Logistic regression
3. Auto-regressive models
4. Panel regression

# Introduction
Describe the background and motivation of your problem.

Finding reliable ways to predict stock prices is a difficult exercise and the methods used often yield unsatisfactory results. A lot of stock market data is readily available and it is important to find significant predictors variables amongst all the options. If we believe that the price of a stock, or the direction of its movement can be predicted using recent information then we can look at variables associated with recent movements with indicators such as RSI or moving averages as well as intraday variatioins in price via the OHLC prices.

If on other the hand we believe that trends and long term movements are a better way to predict prices then time series analysis can be used. The analysist consist of comparing a series of prices such as the close price of a stock with a lagged or shifted version of itself in orde to discern any temporal correlations. This technique requires certain assumptions to be met that require transformations of the data. Through different modeling approaches and parameter tuning, we can prediction what future prices will be to a certain degree of confidence. 


# Literature Review
Discuss how other researchers have addressed similar problems, what their achievements are, and what the advantage and drawbacks of each reviewed approach are. Explain how your investigation is similar or different to the state-of-the- art. Please cite the relevant papers where appropriate.


# Methodology
Discuss the key aspects of your problem, data set and regression model(s). Given that you are working on real-world data, explain at a high-level your exploratory data analysis, how you prepared the data for regression modeling, your process for building regression models, and your model selection.

- EDA showing basic time series and indicators
- Data preparation: create new variables? created an up/down indicator
- Models to try


# Experimentation and Results
Describe the specifics of what you did (data exploration, data preparation, model building, model selection, model evaluation, etc.), and what you found out (statistical analyses, interpretation and discussion of the results, etc.).

### Data Description

The variables description are as follows:

| Variable          | Description                                                                                                                                                                         
|-------------------|---------------------------
| date              | Trading Date                                                                                             
| open              | Price of the stock at market open                                                                                                                                                              
| high              | Highest price reached in the trade day                                                                                                     
| low               | Lowest price reached in the trade day                                                                                                      
| close             | Price of the stock at market close      
| volume            | Number of shares traded 	
| unadjustedVolume  | Volume for stocks, unadjusted by stock splits                                                                                                                      
| change           	| Change in closing price from prior trade day close                                                                                                                       
| changePercent     | Percentage change in closing price from prior trade day close                                                                                                                      
| vwap              | Volume weighted average price (VWAP) is the ratio of the value traded to total volume traded                                                                                                          
| label           	| Trading Date                                                                                                                       
| changeOverTime    | Percent change of each interval relative to first value. Useful for comparing multiple stocks.                                                                                                                         
| ticker            | Abbreviation used to uniquely identify publicly traded shares

```{r message=FALSE, warning=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/salma71/Data_621/master/Project_Proposal/stocks_combined.csv')
tickers <- read_csv('https://raw.githubusercontent.com/salma71/Data_621/master/Project_Proposal/tickers.csv')
skim(dataset)
```

```{r}
head(dataset)
```


### Data Processing

```{r}
dataset$date <- as.Date(dataset$date, format="%m/%d/%Y")
```

### Data Exploration

```{r}
AAPL <- dataset %>% filter(ticker=='AAPL') 
AAPL %>% ggplot(aes(x=date,y=close)) + geom_line()
```





#### Data Transformation for Logistic Model

1. Modify changePercent/change to be our binary target variable
2. Create predictor variables that are not stock dependent like price
- MACD and sig
- Momentum_5 day change % (week)
- Momentum_20 day change % (month)
- RSI
- Volume change

3. Remove observations containing NAs generated by diffential variables (first 26+9 rows as determined by MACD)

```{r}

AAPL_full <- AAPL %>% mutate(macd=MACD(Cl(AAPL), nFast=12, nSlow=26, nSig=9, maType=SMA)[,1], 
                        m5=momentum(AAPL$close, n=5), 
                        m20=momentum(AAPL$close, n=20), 
                        rsi=RSI(AAPL$close, n=14),
                        vol1=ROC(AAPL$volume, n=1),
                        vol5=ROC(AAPL$volume, n=5)) %>%
                        drop_na() %>%
                      select(-c(date,ticker,label, changeOverTime))

head(AAPL_full,10)
```

```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
AAPL_full %>% 
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_histogram(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```


```{r}
# Create target variable

AAPL_model1_data <- AAPL_full%>% 
                  mutate(TARGET=if_else(shift(close, n=1, fill=NA, type="lead") > close, 1, 0)) %>%
                  #select(-c(date,ticker,label,open,high,low,close,volume,unadjustedVolume,vwap,change,changeOverTime)) %>%
                  drop_na()
                  
AAPL_model1_data$TARGET <- factor(AAPL_model1_data$TARGET)

head(AAPL_model1_data,10)
```

```{r}
library(corrplot)
corr_dataframe <- AAPL_model1_data %>% mutate_if(is.factor, as.numeric)
corr.d <- cor(corr_dataframe)
corr.d[ lower.tri( corr.d, diag = TRUE ) ] <- NA
corrplot( corr.d, type = "upper", diag = FALSE )
```

```{r}
AAPL_model1_data %>% ggplot(aes(x=TARGET)) + geom_histogram(stat="count")
```

### Model 1 (Logistic)

#### M1A Full Model


```{r}
model.full <- glm(TARGET ~ .,
                 data=AAPL_model1_data,
                 family = binomial(link="logit")
                 )
summary(model.full)
```

```{r}
car::vif(model.full)
```

```{r}
glm.probs <- predict(model.full, type="response")
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
results <- tibble(target=AAPL_model1_data$TARGET, pred=glm.pred)
results <- results %>% mutate(pred.class = as.factor(pred), target.class = as.factor(target))
print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
```


#### M1B Small Model

The smaller model leads to a nonsensical results where the two variables cancel each other out. Multi-collinearity issue.

```{r}
model.small <- glm(TARGET ~ .-vol5 -high -rsi -open -vol1-changePercent-low-macd-m20-volume-unadjustedVolume-m5-change,
                 data=AAPL_model1_data,
                 family = binomial(link="logit")
                 )
summary(model.small)
```

```{r}
glm.probs <- predict(model.small, type="response")
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
results <- tibble(target=AAPL_model1_data$TARGET, pred=glm.pred)
results <- results %>% mutate(pred.class = as.factor(pred), target.class = as.factor(target))
print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
```

#### M1C Multicollinearity Removed

```{r}
m1c_data <- AAPL_model1_data
m1c_data$Close2Open <- m1c_data$open - shift(m1c_data$close, n=1, fill=NA, type="lag")
m1c_data$Open2Open <- m1c_data$open - shift(m1c_data$open, n=1, fill=NA, type="lag")

m1c_data_trimmed <- m1c_data %>% select(-c(open,high,low,vwap,change,macd)) %>% drop_na()
```


This suggests removing open, high and low. Use Close-Close, Close-Open Insteadd

```{r}
m1c.full <- glm(TARGET ~ .,
                 data=m1c_data_trimmed,
                 family = binomial(link="logit")
                 )
summary(m1c.full)
```

```{r}
car::vif(m1c.full)
```

```{r}
glm.probs <- predict(m1c.full, type="response")
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
results <- tibble(target=m1c_data_trimmed$TARGET, pred=glm.pred)
results <- results %>% mutate(pred.class = as.factor(pred), target.class = as.factor(target))
print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
```

### Model 2 - Autoregressive Model


```{r}
acf(AAPL$close)
```

```{r}
pacf(AAPL$close)
```


```{r}
t <- AAPL$close[-1]
t_1 <- AAPL$close[-length(AAPL$close)]
m2.lm <- lm(t ~ t_1)
summary(m2.lm)
```

```{r}
plot(t_1, t)
abline(m2.lm, col=3, lwd=2)
```

#### Generalized Least Squares

```{r}
# Assuming 200 trading days in a year
AAPL_ts <- ts(AAPL$close, start=c(2014, 2, 21), end=c(2019, 2, 20), frequency=200)
AAPL_close <- ts(AAPL$close)
```


```{r}
m2.gls <- gls(AAPL_close ~ time(AAPL_close), correlation = corAR1(form=~1))
summary(m2.gls)$tTable
```

```{r}
ar(AAPL_close)
```

```{r}
#m2.gls2 <- gls(AAPL_close ~ time(AAPL_close), correlation = corARMA(p=2))
#summary(m2.gls2)
```

#### ARIMA

Time series needs to be stationary. The higher order differentials have mean 0 but the varaince seems to increase in the last part of the series. Below we series the series differentiated to the first and second order.

```{r}
par(mfrow=c(2,1))
APPLdiff1 <- diff(AAPL$close, differences=1)
APPLdiff2 <- diff(AAPL$close, differences=2)
plot.ts(APPLdiff1)
plot.ts(APPLdiff2)
```

##### Order 1 differential

The order 1 differential shows rhe highest correlation at position 0, while the partial ACF seenms to show periodic behavior with a few significant lags which does not tell us much.

```{r}
par(mfrow=c(1,2))
acf(APPLdiff1, lag.max=20)             
pacf(APPLdiff1, lag.max=20)
acf(APPLdiff1, lag.max=20, plot=FALSE) 
pacf(APPLdiff1, lag.max=20, plot=FALSE)
```

##### Order 2 differential

On the other hand, the order two differential has several meaningful lags we can use, and the partial ACF decays.

```{r}
par(mfrow=c(1,2))
acf(APPLdiff2, lag.max=20)             # plot a correlogram
acf(APPLdiff2, lag.max=20, plot=FALSE) # get the autocorrelation values
pacf(APPLdiff2, lag.max=20)             
pacf(APPLdiff2, lag.max=20, plot=FALSE)
```

The partial correlogram shows that the partial autocorrelations at lags 1-6, 8-11 exceed the significance bounds, are negative, and are slowly decreasing in magnitude with increasing lag. The partial autocorrelations tail off to zero after lag 11.

Since the correlogram is zero after lag 1, and the partial correlogram tails off to zero after lag 6 (or 11), this means that the following ARMA (autoregressive moving average) models are possible for the time series of first differences:

* an ARMA(6 or 11,0) model, that is, an autoregressive model of order p=6, since the partial autocorrelogram is zero after lag 6, and the autocorrelogram tails off to zero (although perhaps too abruptly for this model to be appropriate)
* an ARMA(0,1) model, that is, a moving average model of order q=1, since the autocorrelogram is zero after lag 1 and the partial autocorrelogram tails off to zero
* an ARMA(p,q) model, that is, a mixed model with p and q greater than 0, since the autocorrelogram and partial correlogram tail off to zero (although the correlogram probably tails off to zero too abruptly for this model to be appropriate)

AR(1), differential(2), MA(0)

```{r}
arimadf <- tibble(model=character(), coef=character(), loglik=numeric(), aic=numeric()) 

arimamodels <- function(p,d,q) {
  # A specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order. ARMA(p,q)
  arima.model <- arima(AAPL$close, order = c(p,d,q))
  arima.model_fitted <- AAPL$close - residuals(arima.model)
  df <- tibble(model=paste0(p,d,q), coef=paste0(list(round(arima.model$coef,3)), loglik=arima.model$loglik, aic=arima.model$aic))
  return.list <- list(model=arima.model,fitted=arima.model_fitted, df=df)
  return(return.list)
}
```

```{r}
m120 <- arimamodels(p=1, d=2, q=0)
arimadf <- rbind(arimadf, m120$df)
m021 <- arimamodels(0, 2, 1)
arimadf <- rbind(arimadf, m021$df)
m620 <- arimamodels(6, 2, 0)
arimadf <- rbind(arimadf, m620$df)
m621 <- arimamodels(6, 2, 1)
arimadf <- rbind(arimadf, m621$df)

arimadf
```

```{r}
par(mfrow=c(2,2))
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m120$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m021$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m620$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m621$fitted, type = "l", col = 2, lty = 2)
```



AR(1), differential(2), MA(6)

```{r}
# A specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.
model2.arima021 <- arima(AAPL$close, order = c(0, 2, 1))
model2.arima021_fitted <- AAPL$close - residuals(model2.arima021)
model2.arima021
```

```{r}
plot(AAPL$close, type = "l", col = 4, lty = 1)
points(model2.arima021_fitted, type = "l", col = 2, lty = 2)
```


```{r}
library("forecast")
AAPLforecast <- forecast(m021$model, h=150)
```

```{r}
autoplot(AAPLforecast)
```

### Forecast vs Actual

Using 70% training data

```{r}
AAPLsubset <- AAPL$close[1:round(0.7*length(AAPL$close))] 
model3.arima021 <- arima(AAPLsubset, order = c(0, 2, 1))
model3.arima021_fitted <- AAPLsubset - residuals(model3.arima021)
model3.arima021
```


```{r}
boundary <- round(length(AAPL$close)*.7)
end <- length(AAPL$close)
data <- tibble(date=AAPL$date, price=AAPL$close, prediction = NA, fitted=NA, Low95 = NA,
         Low80 = NA,
         High95 = NA,
         High80 = NA)


AAPLsubsetforecast <- forecast(model3.arima021, h=377)
forecast_df <- fortify(as.data.frame(AAPLsubsetforecast)) %>% as_tibble()
forecast_df <- forecast_df %>%
  rename("Low95" = "Lo 95",
         "Low80" = "Lo 80",
         "High95" = "Hi 95",
         "High80" = "Hi 80",
         "Forecast" = "Point Forecast")

#data[boundary:end,3:7] <- forecast_df

data$fitted <- c(as.vector(model3.arima021_fitted),rep(NA,(end-boundary)))
data$prediction <- c(rep(NA,boundary),forecast_df$Forecast)
data$Low80 <- c(rep(NA,boundary),forecast_df$Low80)
data$High80 <- c(rep(NA,boundary),forecast_df$High80)
data$Low95 <- c(rep(NA,boundary),forecast_df$Low95)
data$High95 <- c(rep(NA,boundary),forecast_df$High95)

tail(data)
```
```{r}
ggplot(data, aes(x = date)) + 
  geom_ribbon(aes(ymin = Low95, ymax = High95, fill = "95%")) +
  geom_ribbon(aes(ymin = Low80, ymax = High80, fill = "80%")) +
  geom_point(aes(y = price, colour = "price"), size = 1) +
  geom_line(aes(y = price, group = 1, colour = "price"), 
            linetype = "dotted", size = 0.75) +
  geom_line(aes(y = fitted, group = 2, colour = "fitted"), size = 0.75) +
  geom_line(aes(y = prediction, group = 3, colour = "prediction"), size = 0.75) +
  scale_x_date(breaks = scales::pretty_breaks(), date_labels = "%b %y") +
  scale_colour_brewer(name = "Legend", type = "qual", palette = "Dark2") +
  scale_fill_brewer(name = "Intervals") +
  guides(colour = guide_legend(order = 1), fill = guide_legend(order = 2)) +
  theme_bw(base_size = 14)
```


### Model 3




### Data Imputation

Example for citing @kulkarni_time_2020

### Model Building



### Model Evaluation



### Results


# Discussion and Conclusion
Conclude your findings, limitations, and suggest areas for future work.


\newpage
# Appendix A. Figures


\newpage
# Appendix B. Tables

\newpage
# Appendix C. Code

```{r Code, echo=T, eval=F}
install_load <- function(pkg){
  # Load packages & Install them if needed.
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
```

\newpage
# References


