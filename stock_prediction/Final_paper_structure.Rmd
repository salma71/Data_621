---
title: "DATA 621 Final Project - Stock Predictions Methods"
author: |
  | **Dhairav Chhatbar, Mael Illien, Salma Elshahawy**
  | DATA 621, Master of Science in Data Science,
  | City University of New York
link-citations: yes
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---
nocite: |
  @wickham_ggplot2_2009, @RCoreTeam, @Chang2015
bibliography: citations.bib
---


```{r options_pkgs, echo=F, warning=F, message=F, results=F}
# knitr::opts_chunk$set(error = F, message = F, # tidy = T,
#                       cache = T, warning = T, 
#                       results = 'hide', # suppress code output
#                       echo = F,         # suppress code
#                       fig.show = 'hide' # suppress plots
#                       )
library(readr)
library(skimr)
library(knitcitations)
library(pander)
library(tidyverse)
library(corrplot)
library(quantmod)
library(data.table)
library(car)
library(caret)
library(nlme)
library(kableExtra)
library(tseries)
```

# Abstract
*Use 250 words or less to summarize your problem, methodology, and major outcomes.*

Problem:
Methodology:
Major Outcomes:

There are a few ways of predicting stock prices. From blind guessing to machine learning, many techniques have been tested to try to achieve decent results. Capturing the complexity of the market in a model is a daunting task but it can be broken down in a number of appraoches. 

Since predicting the exact percent increase of a stock the next day is difficult, we can instead try instead to predict the direction of the movement. Alternatively, we can look at the trends and seasonality in the data and try to predict with some level of confidence where the price would like in an arbitrary number of days.

In this paper, we implemented models using logistic regression, panel regresssion, generalized least squares (maybe) and autoregressive models using ARIMA.

The logistic models which try to predict the direction of the next day's movement yielded poor results. Hardly any of the predictor variable were significant, and the classification accuracies were around 52% which is hardly better than guessing. On the other hand, the auto-regressive models that predict subsequent values of a stock time series were of greater use. This required finding a suitable combination of parameters to fit the data closely. 

The panel models




# Key Words
*Select a few key words (up to five) related to your work.*

1. Time series
2. Logistic regression
3. Auto-regressive models
4. Panel regression

# Introduction
*Describe the background and motivation of your problem.*

Finding reliable ways to predict stock prices is a difficult exercise and the methods used often yield unsatisfactory results. A lot of stock market data is readily available and it is important to find significant predictors variables amongst all the options. If we believe that the price of a stock, or the direction of its movement can be predicted using recent information then we can look at variables associated with recent movements with indicators such as RSI or moving averages as well as intraday variatioins in price via the OHLC prices.

If on other the hand we believe that trends and long term movements are a better way to predict prices then time series analysis can be used. The analysist consist of comparing a series of prices such as the close price of a stock with a lagged or shifted version of itself in orde to discern any temporal correlations. This technique requires certain assumptions to be met that require transformations of the data. Through different modeling approaches and parameter tuning, we can prediction what future prices will be to a certain degree of confidence. 


# Literature Review
*Discuss how other researchers have addressed similar problems, what their achievements are, and what the advantage and drawbacks of each reviewed approach are. Explain how your investigation is similar or different to the state-of-the- art. Please cite the relevant papers where appropriate.*


# Methodology
*Discuss the key aspects of your problem, data set and regression model(s). Given that you are working on real-world data, explain at a high-level your exploratory data analysis, how you prepared the data for regression modeling, your process for building regression models, and your model selection.*

- EDA showing basic time series and indicators
- Data preparation: create new variables? created an up/down indicator
- Models to try

The dataset is composed of a variety of stocks. Fur the purpose of brevity, the data exploration will be limited to a single stock, Apple (AAPL). The dataset contains typical pricing variables and it is also augmented with engineered features which are believe to have predictive power. The engineered features used are technical indicators such as the RSI (Relative Strength Indicator), the MACD (Moving Average Convergence Divergence), or simple moving averages.

The logistic models require the creation of a binary target variable. It is calculated by comparing the price of the stock with the price on the following day. If the difference is positive, the target is avaluated as 1, and 0 otherwise. The logistic models are built using Generalized Linear Models suited for the exponential family of distributions (in this case binomial) with the help of the `glm` function.

The autoregressive models only makes use of the price time series and fits a model that explains a given time series based on its previous values and how correlated it is to its lags (previous values). Auto Regressive Integrated Moving Average models (ARIMA) are 3 parameter models that can be used to forecast future values. The parameter p represent the AR (auto regressive) term, d is the MA (moving average) term and I (integration) is the order of differentiation required to make the series stationary.


# Experimentation and Results
Describe the specifics of what you did (data exploration, data preparation, model building, model selection, model evaluation, etc.), and what you found out (statistical analyses, interpretation and discussion of the results, etc.).

### Data Description

The variables description are as follows:

| Variable          | Description                                                                                                                                                                         
|-------------------|---------------------------
| date              | Trading Date                                                                                             
| open              | Price of the stock at market open                                                                                                                                                              
| high              | Highest price reached in the trade day                                                                                                     
| low               | Lowest price reached in the trade day                                                                                                      
| close             | Price of the stock at market close      
| volume            | Number of shares traded 	
| unadjustedVolume  | Volume for stocks, unadjusted by stock splits                                                                                                                      
| change           	| Change in closing price from prior trade day close                                                                                                                       
| changePercent     | Percentage change in closing price from prior trade day close                                                                                                                      
| vwap              | Volume weighted average price (VWAP) is the ratio of the value traded to total volume traded                                                                                                          
| label           	| Trading Date                                                                                                                       
| changeOverTime    | Percent change of each interval relative to first value. Useful for comparing multiple stocks.                                                                                                                         
| ticker            | Abbreviation used to uniquely identify publicly traded shares

```{r message=FALSE, warning=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/salma71/Data_621/master/Project_Proposal/stocks_combined.csv')
tickers <- read_csv('https://raw.githubusercontent.com/salma71/Data_621/master/Project_Proposal/tickers.csv')
```

The tickers available in this dataset are listed below. We will be focusing on the AAPL stock which is described below.

```{r}
tickers
```

```{r}
skim(dataset)
```

```{r}
head(dataset)
```


### Data Processing

The only necessary processing of the data is formatting the date column appropriately.

```{r}
dataset$date <- as.Date(dataset$date, format="%m/%d/%Y")
```

#### Feature Engineering

The dataset is augmented with technicals indicators from the `ta-lib` package and the target variable for the logistic model is created. Observations are required to be dropped where NA values are introduced due to the shifting required by the window functions. We are interested in rates of change instead of value in order to be able to any stocks, which will not have the same price.

- *TARGET*: the binaby target variable for the logistic model 
- *MACD*: the MACD signal (only) using the typical 12 and 26 window sizes
- *m5*: momentum indicator over the last 5 days (moving average of one trading week)
- *m20*: momentum indicator over the last 20 days (moving average of one trading month)
- *vol1*: rate of change of the volume over the last day
- *vol5*: rate of change of the volume over the last five dasy (one trading week)


```{r}
AAPL <- dataset %>% filter(ticker=='AAPL') 

# Augment data frame
AAPL_full <- AAPL %>% mutate(macd=MACD(Cl(AAPL), nFast=12, nSlow=26, nSig=9, maType=SMA)[,1], 
                        m5=momentum(AAPL$close, n=5), 
                        m20=momentum(AAPL$close, n=20), 
                        rsi=RSI(AAPL$close, n=14),
                        vol1=ROC(AAPL$volume, n=1),
                        vol5=ROC(AAPL$volume, n=5)) %>%
                        drop_na() %>%
                      select(-c(date,ticker,label, changeOverTime))

# Create target variable
AAPL_model1_data <- AAPL_full%>% 
                  mutate(TARGET=if_else(shift(close, n=1, fill=NA, type="lead") > close, 1, 0)) %>%
                  #select(-c(date,ticker,label,open,high,low,close,volume,unadjustedVolume,vwap,change,changeOverTime)) %>%
                  drop_na()
                  
AAPL_model1_data$TARGET <- factor(AAPL_model1_data$TARGET)
```

```{r}
head(AAPL_model1_data,10)
```

### Data Exploration

The stock price for AAPL over the years are risen steadily. There are two important periods of decline but the overall trends remains positive. These periods of decline should provide balance to the dataset. 

```{r}
AAPL %>% ggplot(aes(x=date,y=close)) + geom_line() + ggtitle('AAPL Close Price')
```

The distributions of the predictor variables are mostly centered. The variables related to volume are skewed to the right tail. The variables close, high and low

```{r variables_distribution, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
AAPL_full %>% 
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(x= value)) + 
  geom_histogram(fill='pink') + 
  facet_wrap(~key, scales = 'free')
```

```{r}
library(corrplot)
corr_dataframe <- AAPL_model1_data %>% mutate_if(is.factor, as.numeric)
corr.d <- cor(corr_dataframe)
corr.d[ lower.tri( corr.d, diag = TRUE ) ] <- NA
corrplot( corr.d, type = "upper", diag = FALSE )
```

```{r}
AAPL_model1_data %>% ggplot(aes(x=TARGET)) + geom_histogram(stat="count")
```

### Modeling

#### Model 1 (Logistic)

```{r}
# Initialize a df that will store the metrics of models
models.df <- tibble(id=character(), formula=character(), res.deviance=numeric(), null.deviance=numeric(),
                 aic=numeric(), accuracy=numeric(), sensitivity=numeric(), specificity=numeric(),
                precision.deviance=numeric(), stringsAsFactors=FALSE) 
```


```{r include=FALSE}
# A function to extract the relevant metrics from the summary and confusion matrix
build_model <- function(id, formula, data) {
  glm.fit <- glm(formula, data=data, family=binomial(link="logit"))
  print(summary(glm.fit))
  glm.probs <- predict(glm.fit, type="response")
  # Confirm the 0.5 threshold
  glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
  results <- tibble(target=data$TARGET, pred=glm.pred)
  results <- results %>%
    mutate(pred.class = as.factor(pred), target.class = as.factor(target))
  
  #print(confusionMatrix(results$pred.class,results$target.class, positive = "1"))
  
  acc <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$overall['Accuracy']
  sens <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Sensitivity']
  spec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Specificity']
  prec <- confusionMatrix(results$pred.class,results$target.class, positive = "1")$byClass['Precision']
  res.deviance <- glm.fit$deviance
  null.deviance <- glm.fit$null.deviance  
  aic <- glm.fit$aic
  metrics <- list(res.deviance=res.deviance, null.deviance=null.deviance,aic=aic, accuracy=acc, sensitivity=sens, specificity=spec, precision=prec)
  metrics <- lapply(metrics, round, 3)
  
  #plot(roc(results$target.class,glm.probs), print.auc = TRUE)
  model.df <- tibble(id=id, formula=formula, res.deviance=metrics$res.deviance, null.deviance=metrics$null.deviance, 
                         aic=metrics$aic, accuracy=metrics$accuracy, sensitivity=metrics$sensitivity, specificity=metrics$specificity, precision=metrics$precision)
  model.list <- list(model=glm.fit, df_info=model.df)
  return(model.list)
}
```

#### M1A Full Model

The first logistic model yields unsatisfactory results. Only the `close` variable is significant. It is revealed that a lot of the variables are  multicollinear. This makes sense given that a number of variables are related to the price are generally move together (close, open, low, high), and some are derived from other variables. The model should be reduced by removing the least significant predictors until the model is significant.

```{r}
model.full <- build_model('model.full', "TARGET ~ .", data = AAPL_model1_data)
models.df <- rbind(models.df,model.full$df_info)
#summary(model.full)
```

```{r echo=FALSE}
print("VIF:")
car::vif(model.full$model)
```


#### M1B Small Model

Reducing the model down to only the significant predictors leads to a nonsensical results where the two variables cancel each other out.

```{r}
model.small <- build_model('model.small', "TARGET ~ .-vol5 -high -rsi -open -vol1-changePercent-low-macd-m20-volume-unadjustedVolume-m5-change", data = AAPL_model1_data)
models.df <- rbind(models.df,model.small$df_info)
summary(model.small)
```

#### M1C Multicollinearity Removed

This model removes a lot of the predictors (open,high,low,vwap,change,macd) but adds `Close2Open` and `Open2Open` in order to  try to retain some information from the variables that were dropped. The removed variables were determined to be too correlated and we retained only the predictors with Variance Inflation Factors below an allowable treshold as seen below. Unfortunately, this model also lacks significance.

```{r}
m1c_data <- AAPL_model1_data
m1c_data$Close2Open <- m1c_data$open - shift(m1c_data$close, n=1, fill=NA, type="lag")
m1c_data$Open2Open <- m1c_data$open - shift(m1c_data$open, n=1, fill=NA, type="lag")
m1c_data_trimmed <- m1c_data %>% select(-c(open,high,low,vwap,change,macd)) %>% drop_na()

model.m1c  <- build_model('model.m1c', "TARGET ~ .", data = m1c_data_trimmed )
models.df <- rbind(models.df,model.m1c$df_info)
summary(model.m1c)
```

```{r}
car::vif(model.m1c$model)
```

#### Logistic Model Results

The results of the logistic models are displayed below. The in-sample testing accuracy is fairly low at 52%. No model has a particular edge so we proceed to move on to other types of model. This exercise could be enhanced by using data from other stocks by removing all the variables that are stock dependent. Additionally, more target could be used such as the the direction of the price movement over 5 days or 20 days. 

```{r}
models.df
```

#### Model 2 - Autoregressive Model

In this section we drop all the predictors but the close price of the time series. We only use information contain the series itself to fit a model and make predictions. The AAPL stock price had an underlying trend but large fluctuations. We need to verify if certain conditions are met in order to model this time series. We first look at the autocorrelation of the series and see that the series is highly correlated with its previous values with a significant lag. We also consider the PACF which removes variations explained by earlier lags so we get only the relevant features.

Identification of an AR model is often done using the PACF while MA models use the ACF. The order of the model is determined by the numer of significant lags taken into account. The PACF suggests ar AR(1) model and a regression line of time t onto time t-1 shows a close linear fit and a highly statistical coefficient of approximately 1 for `t-1`. However, the slowly decaying ACF suggests that the series is not stationary and that some degree of differentiation might be required to stabilize the mean.


```{r}
par(mfrow=c(3,1))
acf(AAPL$close)
pacf(AAPL$close)

t <- AAPL$close[-1]
t_1 <- AAPL$close[-length(AAPL$close)]
m2.lm <- lm(t ~ t_1)

plot(t_1, t)
abline(m2.lm, col=3, lwd=2)
```

```{r}
summary(m2.lm)
```

We use the generalized least squares method to obtain more information on the mode. We specify an error correlation structure of order 1. 

```{r}
m2.gls <- gls(AAPL_close ~ time(AAPL_close), correlation = corAR1(form=~1))
summary(m2.gls)
```

We test the justification of the added autoregressive structure using a likelyhood ratio test. With a very small p value, we can reject the null hypothesis that the added term is not necessary.

```{r}
m2.gls.0 <- update(m2.gls, correlation=NULL)
anova(m2.gls, m2.gls.0) # AR(1) vs Uncorrelated errors
```


Fitting an AR(1) model using the `ar` function confirms the coefficient of approxmiately 1 (0.997).

```{r}
m2.ar1 <- ar(AAPL_close)
m2.ar1_fitted <- AAPL$close - residuals(m2.ar1)
m2.ar1
```

```{r}
plot(AAPL$close, type = "l", col = 4, lty = 1)
points(m2.ar1_fitted, type = "l", col = 2, lty = 2)
```

We noted earlier that the decay structure of the ACF suggested that the series is not stationary. We verify this claim using the Dickey Fuller tests below. The null hypothesis that the series is not stationary is not rejected for the original series, but rejected for the once differentiated series. Therefore, the original series is not stationary and should be differentiated to stabilize the mean.


```{r}
print('DFT on time series:')
adf.test(AAPL_close)
print('DFT on differentiated time series:')
adf.test(diff(AAPL$close, differences=1))
```

#### ARIMA

Time series needs to be stationary. The higher order differentials have mean 0 but the varaince seems to increase in the last part of the series. Below we series the series differentiated to the first and second order.

Stabilize the mean

```{r}
par(mfrow=c(2,1))
APPLdiff1 <- diff(AAPL$close, differences=1)
APPLdiff2 <- diff(AAPL$close, differences=2)
plot.ts(APPLdiff1)
plot.ts(APPLdiff2)
```

##### Order 1 differential

The order 1 differential shows rhe highest correlation at position 0, while the partial ACF seenms to show periodic behavior with a few significant lags which does not tell us much.

```{r}
par(mfrow=c(1,2))
acf(APPLdiff1, lag.max=20)             
pacf(APPLdiff1, lag.max=20)
acf(APPLdiff1, lag.max=20, plot=FALSE) 
pacf(APPLdiff1, lag.max=20, plot=FALSE)
```

##### Order 2 differential

On the other hand, the order two differential has several meaningful lags we can use, and the partial ACF decays.

```{r}
par(mfrow=c(1,2))
acf(APPLdiff2, lag.max=20)             # plot a correlogram
acf(APPLdiff2, lag.max=20, plot=FALSE) # get the autocorrelation values
pacf(APPLdiff2, lag.max=20)             
pacf(APPLdiff2, lag.max=20, plot=FALSE)
```

The partial correlogram shows that the partial autocorrelations at lags 1-6, 8-11 exceed the significance bounds, are negative, and are slowly decreasing in magnitude with increasing lag. The partial autocorrelations tail off to zero after lag 11.

Since the correlogram is zero after lag 1, and the partial correlogram tails off to zero after lag 6 (or 11), this means that the following ARMA (autoregressive moving average) models are possible for the time series of first differences:

* an ARMA(6 or 11,0) model, that is, an autoregressive model of order p=6, since the partial autocorrelogram is zero after lag 6, and the autocorrelogram tails off to zero (although perhaps too abruptly for this model to be appropriate)
* an ARMA(0,1) model, that is, a moving average model of order q=1, since the autocorrelogram is zero after lag 1 and the partial autocorrelogram tails off to zero
* an ARMA(p,q) model, that is, a mixed model with p and q greater than 0, since the autocorrelogram and partial correlogram tail off to zero (although the correlogram probably tails off to zero too abruptly for this model to be appropriate)

AR(1), differential(2), MA(0)

```{r}
arimadf <- tibble(model=character(), coef=character(), loglik=numeric(), aic=numeric()) 

arimamodels <- function(p,d,q) {
  # A specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order. ARMA(p,q)
  arima.model <- arima(AAPL$close, order = c(p,d,q))
  arima.model_fitted <- AAPL$close - residuals(arima.model)
  df <- tibble(model=paste0(p,d,q), coef=paste0(list(round(arima.model$coef,3)), loglik=arima.model$loglik, aic=arima.model$aic))
  return.list <- list(model=arima.model,fitted=arima.model_fitted, df=df)
  return(return.list)
}
```

```{r}
m120 <- arimamodels(p=1, d=2, q=0)
arimadf <- rbind(arimadf, m120$df)
m021 <- arimamodels(0, 2, 1)
arimadf <- rbind(arimadf, m021$df)
m620 <- arimamodels(6, 2, 0)
arimadf <- rbind(arimadf, m620$df)
m621 <- arimamodels(6, 2, 1)
arimadf <- rbind(arimadf, m621$df)

arimadf
```

```{r}
par(mfrow=c(2,2))
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m120$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m021$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m620$fitted, type = "l", col = 2, lty = 2)
#plot(AAPL$close, type = "l", col = 4, lty = 1)
plot(m621$fitted, type = "l", col = 2, lty = 2)
```



AR(1), differential(2), MA(6)

```{r}
# A specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.
model2.arima021 <- arima(AAPL$close, order = c(0, 2, 1))
model2.arima021_fitted <- AAPL$close - residuals(model2.arima021)
model2.arima021
```

```{r}
plot(AAPL$close, type = "l", col = 4, lty = 1)
points(model2.arima021_fitted, type = "l", col = 2, lty = 2)
```


```{r}
library("forecast")
AAPLforecast <- forecast(m021$model, h=150)
```

```{r}
autoplot(AAPLforecast)
```

### Forecast vs Actual

Using 70% training data

```{r}
AAPLsubset <- AAPL$close[1:round(0.7*length(AAPL$close))] 
model3.arima021 <- arima(AAPLsubset, order = c(0, 2, 1))
model3.arima021_fitted <- AAPLsubset - residuals(model3.arima021)
model3.arima021
```


```{r}
boundary <- round(length(AAPL$close)*.7)
end <- length(AAPL$close)
data <- tibble(date=AAPL$date, price=AAPL$close, prediction = NA, fitted=NA, Low95 = NA,
         Low80 = NA,
         High95 = NA,
         High80 = NA)


AAPLsubsetforecast <- forecast(model3.arima021, h=377)
forecast_df <- fortify(as.data.frame(AAPLsubsetforecast)) %>% as_tibble()
forecast_df <- forecast_df %>%
  rename("Low95" = "Lo 95",
         "Low80" = "Lo 80",
         "High95" = "Hi 95",
         "High80" = "Hi 80",
         "Forecast" = "Point Forecast")

#data[boundary:end,3:7] <- forecast_df

data$fitted <- c(as.vector(model3.arima021_fitted),rep(NA,(end-boundary)))
data$prediction <- c(rep(NA,boundary),forecast_df$Forecast)
data$Low80 <- c(rep(NA,boundary),forecast_df$Low80)
data$High80 <- c(rep(NA,boundary),forecast_df$High80)
data$Low95 <- c(rep(NA,boundary),forecast_df$Low95)
data$High95 <- c(rep(NA,boundary),forecast_df$High95)

tail(data)
```
```{r}
ggplot(data, aes(x = date)) + 
  geom_ribbon(aes(ymin = Low95, ymax = High95, fill = "95%")) +
  geom_ribbon(aes(ymin = Low80, ymax = High80, fill = "80%")) +
  geom_point(aes(y = price, colour = "price"), size = 1) +
  geom_line(aes(y = price, group = 1, colour = "price"), 
            linetype = "dotted", size = 0.75) +
  geom_line(aes(y = fitted, group = 2, colour = "fitted"), size = 0.75) +
  geom_line(aes(y = prediction, group = 3, colour = "prediction"), size = 0.75) +
  scale_x_date(breaks = scales::pretty_breaks(), date_labels = "%b %y") +
  scale_colour_brewer(name = "Legend", type = "qual", palette = "Dark2") +
  scale_fill_brewer(name = "Intervals") +
  guides(colour = guide_legend(order = 1), fill = guide_legend(order = 2)) +
  theme_bw(base_size = 14)
```


### Model 3




### Data Imputation

Example for citing @kulkarni_time_2020

### Model Building



### Model Evaluation



### Results


# Discussion and Conclusion
Conclude your findings, limitations, and suggest areas for future work.


\newpage
# Appendix A. Figures


\newpage
# Appendix B. Tables

\newpage
# Appendix C. Code

```{r Code, echo=T, eval=F}
install_load <- function(pkg){
  # Load packages & Install them if needed.
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
```

\newpage
# References


